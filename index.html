<!--doctype html--><html></html><html lang="en"><head><title>BI TECH CP303 - Data Mining</title><meta charset="UTF-8"/><meta name="fragment" content="!"/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/><link rel="stylesheet" href="css/foundation.min.css"/><link rel="stylesheet" href="css/reveal.css"/><!-- Theme for syntax highlighting--><link rel="stylesheet" href="lib/css/zenburn.css"/><link rel="stylesheet" href="css/theme/data-mining.css"/></head><body><header class="full-width header-area"><div class="row"><div class="large-12 columns"></div></div></header><div id="main-nav"></div><div class="contain-to-grid navigation-area"><div class="row"><div class="large-12 columns"><nav class="top-bar"><ul class="title-area"><img src="images/uw_logo.svg" id="logo-image"/><li class="toggle-topbar menu-icon"><a href="#">nav            </a></li></ul><section class="top-bar-section"><ul class="right"><li class="has-dropdown not-click"><a href="#" class="active">Topic</a><ul class="dropdown"><li class="active"><a href="#/intro" class="active">Intro to data mining and R</a></li><li class="active"><a href="#/regression" class="active">Linear regression</a></li><li class="active"><a href="#/selection" class="active">Feature selection </a></li><li class="active"><a href="#/logistic" class="active">Logistic regression</a></li><li class="active"><a href="#/trees" class="active">Classification Trees </a></li><li class="active"><a href="#/rules" class="active">Association Rules</a></li><li class="active"><a href="#/clustering" class="active">Clustering</a></li><li class="active"><a href="#/share" class="active">Markdown</a></li></ul></li><li class="has-dropdown not-click"><a href="#" class="active">About </a><ul class="dropdown"><li class="active"><a href="#/how_to" class="active">how to use these slides </a></li><li class="active"><a href="#/textbook" class="active">textbook</a></li><li class="active"><a href="#/author" class="active">the author</a></li><li class="active"><a href="#/acknowledgements" class="active">acknowledgements</a></li></ul></li><li class="active not-click"><a href="https://github.com/erinshellman/data-mining-starter-kit/issues" class="active">Report a bug</a></li></ul></section></nav></div></div></div><div class="reveal"><div class="slides"><section class="title-slide"><h1>BI Tech CP303 - Data Mining</h1><h2>Erin Shellman - <a href="mailto:shellman@uw.edu" class="active">shellman@uw.edu</a></h2><h2><b>TA</b>: Bryan Mayer - <a href="mailto:mayerbry@uw.edu" class="active">mayerbry@uw.edu</a></h2><p>Welcome! These are lecture notes that accompany an applied course on data mining. Read the <a href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/README.md">syllabus</a> to find details about assignments and due dates.</p></section><section id="textbook" data-transition-speed="fast"><h1>Textbooks</h1><p>There is no book required for this course, however there are two great texts available for free online.</p><div class="flex-row"><a href="http://www.mmds.org/" class="left"><img src="images/mmds.jpg" style="height: 40%"/></a><a href="http://www.statsoft.com/Textbook/Data-Mining-Techniques" class="right"><img src="images/hill_lewicki.jpg" style="height: 40%"/></a></div></section><section data-transition-speed="fast"><h1>Course Philosophy</h1><p>Data mining is an applied skill and this course focuses on applications. Each class is a blend of concepts and applications using the programming language R. There are three projects, each consisting of a data mining analysis to solve a business problem and a brief written report of your findings.</p><p>By the end of this course, you'll be able to approach unfamiliar data and quickly find and communicate results.</p></section><section data-transition-speed="fast"><section data-transition-speed="fast"><h1>Navigating the notes</h1><p>The notes progress from left to right, and for those interested in exploring more about a topic sometimes there's bonus material in the up, down direction.</p></section><section data-transition-speed="fast"><h1>Just like this!</h1></section></section><section data-transition-speed="fast"><h2>Traverse the topics </h2><p>You can also locate slides by topic using the navigation bar at the top.</p></section><section data-transition-speed="fast"><h2>Bookmark and check back often!</h2><p>Data technology is a rapidly growing and changing industry. I chose <a href="http://lab.hakim.se/reveal-js/#/">this format</a> for course notes so the slides could be easily maintained and up-to-date. As you apply these skills outside of class, I hope these notes can be a valuable reference.</p></section><section id="how_to" data-transition-speed="fast"><h2>Notes improved with the support of viewers like you!</h2><p>These slides are a work in progress and I need your help to make them rule. If you see an error or think something is unclear or omitted, please <a href="https://github.com/erinshellman/data-mining-starter-kit/issues">report it!</a></p><p>I'll do my best to keep the slides as updated and accurate as possible.</p></section><section id="intro" data-background="images/data_miner1.jpg" data-background-size="cover" class="title-slide"><h1>Introduction to Data Mining with R</h1><h3>BI Tech CP303 - Data Mining</h3><h3><a href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/intro-R/introduction-to-r.pdf">R Tutorial</a></h3></section><section data-transition-speed="fast"><section data-background="images/data-explosion.png" data-background-size="100%"><!--h1 We're creating data faster than we can understand it.--></section><section data-transition-speed="fast"><h2>We are inundated with data</h2><p>Governments, corporations, scientists, and consumers are creating and collecting more data than ever before. The unprecendented availability of data has transformed the modern economy and, for many, the human condition. Using clickstream technology, the modern advertiser can follow would-be customers as they browse online. These marketers are searching for clues about your preferences and shopping habits, evidence of your family composition, and even upcoming events in your life, all for the purposes of personalization. On an individual level, the <a href="https://en.wikipedia.org/wiki/Internet_of_Things">Internet of Things</a> allows us to continually monitor our bodies with Fitbits, Fuel bands, and Jawbone, and our homes with products like Nest. </p><p>Not only do consumers personally generate data, but they <i>share</i> it with others on social media. Similarly the <a href="https://ropensci.org/blog/2014/11/10/open-data-growth/">biological</a> and <a href="http://earthobservatory.nasa.gov/Features/LandsatBigData/">earth</a> sciences are exploding with data as the cost of collecting it has plummetted. And the government, of course, wants all the data. </p></section><section data-transition-speed="fast"><h2>We are inundated with data</h2><p>But that's just it, we're <i>collecting</i> data in faster than ever in history, but our growth of <i>understanding</i> of these data has not kept pace. </p></section></section><section data-transition-speed="fast"><section data-background="images/data-variety.png" data-background-size="contain"></section><!--h1 Data are more heterogenous than ever.--><section data-transition-speed="fast"><h2>Data representation and storage is more heterogeneous than ever</h2><p>Not only are we generating more data faster, we're developing more ways to represent and store data. Marketing personalization combines disperate data about a single person or household: demographics, transactional histories, social media activities and clickstream. Personalizatin efforts require the ability to ingest and combine data from many sources across many formats. Graph databases like <a href="http://thinkaurelius.github.io/titan/">Titan</a> or <a href="http://neo4j.com/">neo4j</a> represent complex interactions as network graphs and analysts query the network with traversal patterns. Algorithms like mapreduce on the other hand, revolutionized the way we query <a href="http://en.wikipedia.org/wiki/Unstructured_data">unstructured data</a>, like text and weblogs.</p></section></section><section data-transition-speed="fast"><h2>We need new tools</h2><p>In the context of big, heterogeneous data, the number of candidate features is too large to explore with traditional analysis techniques. Further, the volume of data create new challenges that analyists historically haven't dealt with such as</p><ul><li>computational efficiency, <i>i.e.</i> speed</li><li>memory use and allocation, <i>i.e.</i> computing resources</li><li>data pipelines and integrity</li><li>the ethical consequences of findings</li></ul></section><section id="what_is" data-transition-speed="fast"><section data-transition-speed="fast"><h2>Data mining is a process of knowledge discovery </h2><img src="images/data_mining_process.png" style="height: 70%" class="right"/><p>Data Mining is the process of identifying patterns in large volumes of data <u>for the purposes of prediction</u>. Realistically, data cleaning and exploratory analysis will occupy the vast majority of your time with the rest left over for model discovery. Model deployment is optional and depends on the result of the first three steps. After completing the analysis you might find that long-term adoption of your model isn't worthwhile.</p></section><section data-transition-speed="fast"><h2>Additional definitions</h2><blockquote>Data mining is the extraction of implicit, previously unknown, and potentially useful information from data. <a href="http://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569">Witten, Frank and Hall</a></blockquote><blockquote>The most commonly accepted definition of “data mining” is the discovery of “models” for data." Where 'model' could mean a statistical model, and machine learning model or an algorithm like mapreduce. <a href="http://www.mmds.org/">Leskovec, Rajaraman, Ullman</a></blockquote><blockquote>Data Mining is an analytic process designed to explore data in search of consistent patterns and/or systematic relationships between variables, and then to validate the findings by applying the detected patterns. <a href="http://www.statsoft.com/Textbook/Data-Mining-Techniques">Hill & Lewicki</a></blockquote></section></section><section data-transition-speed="fast"><section data-transition-speed="fast"><h2>How do statistics and machine learning relate?</h2><p>Statistics and machine learning are tools that we employ in our data mining process. Generally the field of statistics is concerned with the analysis and interpretation of data, while the field of machine learning is primarily focused on prediction. Machine learning is less concerned with interpretation and validity of statistical assumptions (to the extent that they don't negatively influence the model's ability to predict).</p></section><section data-transition-speed="fast"><p>To learn more about the different paradigms of inferential statistics and machine learning I highly recommend <a href="http://faculty.smu.edu/tfomby/eco5385/lecture/Breiman%27s%20Two%20Cultures%20paper.pdf">Statistical Modeling: The Two Cultures</a> by Leo Breiman.</p></section></section><section data-transition-speed="fast"><h2>Machine Learning</h2><p><a href="http://en.wikipedia.org/wiki/Machine_learning">Machine learning</a> is a discipline originating in artificial intelligence that creates and investigates algorithms that help machines <i>learn</i> from data. A machine can be said to learn if its behavior changes such that it performs better in the future.</p><p>Applications of machine learning are often in automated systems spam detectors, or recommendation engines.</p></section><section data-transition-speed="fast"><p>You'll find that many of the techniques you've used in an analysis setting are identical to those used in machine learning, for example, linear and logistic regression. The key difference is that in machine learning, we'll judge our model quality by assessing its ability to predict on hold-out data, rather than through goodness-of-fit metrics like $ R^2 $ or AIC.</p></section><section data-transition-speed="fast"><h2>Lets look at some examples</h2><p>Analysis problems generally fall into one of four classes of problem and we're going to try to get some hands-on experience with all four:<ol><li>numerical prediction </li><li>classification (or <a href="http://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a>)</li><li>clustering and <a href="http://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a></li><li>time-series</li></ol></p></section><section data-transition-speed="fast"><h2>Capital Bikeshare </h2><img src="images/crosswalk_lm.png" style="height: 45%" class="right"/><p>In project 1 you'll analyze data from Washington D.C.'s bikeshare program to predict locations of successful bike stations. An model for this application could be to predict the number of bike rentals per hour as a function of the number of restaurants nearby.</p></section><section data-transition-speed="fast"><h2>Bot or not?</h2><p>In project 2, you'll build a classifier to distinguish Twitter bots from humans.</p></section><section data-transition-speed="fast"><h2>Pandora 2.0</h2><p>In project 3, you'll use association rule mining and clustering to build a song recommender using a subset of the Million Song Dataset.</p></section><section data-transition-speed="fast"><h3>Limitations of data mining</h3><p>Like any analytical technique, and arguably more so than traditional analysis, data mining has limitations:<ul><li>many patterns are uninteresting</li><li>patterns can be coincidental or a result of sampling bias</li><li>data are almost always untidy or missing</li></ul></p><p>How do these limitations influence the interpretation and application of the results of a data mining process? What are our standards for belief?</p></section><section><section><h2>Ethical data mining</h2><p>As a result of the volume and speed at which businesses can collect data, conversations about the ethical use of these data are more important than ever. We're used to reading headlines about ethical violations in big data, but often these incidents can be anticipated and avoided.</p></section><section data-transition-speed="fast"><p>Here's a fairly horrifying list of articles about data mining and potential ethical violoations.</p><ul><li><a href="http://www.nytimes.com/roomfordebate/2014/08/06/is-big-data-spreading-inequality/losing-out-on-employment-because-of-big-data-mining">Losing Out on Employment Because of Big Data Mining</a></li><li><a href="http://arstechnica.com/gadgets/2012/05/on-facebook-deleting-an-app-doesnt-delete-your-data-from-their-system/">On Facebook, deleting an app doesn’t delete your data from their system</a></li><li><a href="http://www.theatlantic.com/national/archive/2013/06/americans-fickle-stance-on-data-mining-and-surveillance/276885/">Americans Fickle Stance on Data Mining and Surveillance</a></li><li><a href="http://techcrunch.com/2006/08/06/aol-proudly-releases-massive-amounts-of-user-search-data/">AOL Proudly Releases Massive Amounts of Private Data</a></li><li><a href="http://www.nytimes.com/2006/08/09/technology/09aol.html?pagewanted=all">A Face Is Exposed for AOL Searcher No. 4417749</a></li></ul></section></section><section data-transition-speed="fast"><h2>Ethics case study: Target</h2><p>Take 20 minutes and read the following piece from the New York Times Magazine which describes how Target used transactional data to target expectant mothers for marketing campaigns: <a href="http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html">How Companies Learn Your Secrets</a></p></section><section data-transition-speed="fast"><h2>The business problem:</h2><blockquote><p>Specifically, the marketers said they wanted to send specially designed ads to women in their second trimester, which is when most expectant mothers begin buying all sorts of new things, like prenatal vitamins and maternity clothing. “Can you give us a list?” the marketers asked.</p></blockquote></section><section data-transition-speed="fast"><h2>Discussion questions</h2><p>Put yourself in the role of an analyst at Target.  <ul><li>What data do you need to build a model to predict if a woman is pregnant? </li><li>Is it ethical to merge data from outside of Target's scope in customers' lives (<i>e.g.</i> public birth records).</li><li>Why did people get upset?</li><li>Does the cost of backlash outweigh the benefits of the knowledge?</li><li>How might Target have met the business goals, without losing customer credibility?</li><li>To what degree should Andrew Pole be held accountable for how his analyses were used?</li></ul></p><aside class="notes">Also linked to your Guest ID is demographic information like your age, whether you are married and have kids, which part of town you live in, how long it takes you to drive to the store, your estimated salary, whether you’ve moved recently, what credit cards you carry in your wallet and what Web sites you visit. </aside></section><section data-transition-speed="fast"><h2>How should we think about ethics?</h2><img src="images/ethics.png" style="width: 38%" class="right"/><p>When we are faced with a data mining application, what framework should we use to think about ethics?</p></section><section data-transition-speed="fast"><h2>Ethical decision points</h2><p>Ethical decision points provide a framework for exploring the relationship between what values you hold as individuals—and as members of a common enterprise—and aligning those values with the actions you take in building and managing products and services utilizing big data technologies. <ol><li><b>Inquiry</b>: what are are my organization's values? What are my values?</li><li><b>Analysis</b>: what are the practice standards in my industry?</li><li><b>Articulation</b>: where are there alignments and gaps between our values and proposed data practices?</li><li><b>Action</b>: what must we do to close value alignment gaps?</li></ol></p></section><section data-transition-speed="fast"><h2>Inquiry: what are my values?</h2><p>Target has a <a href="https://corporate.target.com/about/mission-values">values page</a> on their website, but more relevant to this discussion is the <a href="http://www.target.com/spot/privacy-policy">privacy policy</a>.<blockquote>At Target, we want you to know how we collect, use, share and protect information about you. By interacting with Target, you consent to use of information that is collected or submitted as described in this privacy policy. We may change or add to this privacy policy, so we encourage you to review it periodically. To help you track the changes, we include a history of changes below.</blockquote></p></section><section data-transition-speed="fast"><h2>Analysis: what are the standards of my industry?</h2><p>In marketing a great place to start is the Digital Marketing Association's <a href="http://thedma.org/centers-of-excellence/dma-ethics-and-compliance-program/">Ethics and Compliance Program</a> and <a href="http://thedma.org/wp-content/uploads/DMA_Guidelines_January_2014.pdf">Guidelines for ethical business practice</a> the latter of which covers privacy and marketing to children, something Target did whether they knew it or not, in depth.</p></section><section data-transition-speed="fast"><h2>Articulation: are there value gaps? </h2><blockquote>If we send someone a catalog and say, ‘Congratulations on your first child!’ and they’ve never told us they’re pregnant, that’s going to make some people uncomfortable,” Pole told me. “We are very conservative about compliance with all privacy laws. But even if you’re following the law, you can do things where people get queasy.</blockquote><p>The girl was high school age, a fact that, presumably, Target knew. Why then did they choose to proceed with the mailer?</p></section><section data-transition-speed="fast"><section data-transition-speed="fast"><h2>Action: how do we close the value gaps?</h2><blockquote>With the pregnancy products, though, we learned that some women react badly. Then we started mixing in all these ads for things we knew pregnant women would never buy, so the baby ads looked random. We’d put an ad for a lawn mower next to diapers. We’d put a coupon for wineglasses next to infant clothes. That way, it looked like all the products were chosen by chance.</blockquote></section><section data-transition-speed="fast"><p>... and then of course they say this dehumanizing thing.</p><blockquote>And we found out that as long as a pregnant woman thinks she hasn’t been spied on, she’ll use the coupons. She just assumes that everyone else on her block got the same mailer for diapers and cribs. As long as we don’t spook her, it works.</blockquote></section></section><section data-transition-speed="fast"><h2>Ethical behavior requires constant diligence </h2><p>The ethical consequences of data mining isn't a one-time consideration. Everytime we sit down to conduct and analysis we should think about the potential for harm.  We'll return to the discussion of ethics repeatedly throughout the course.</p></section><section id="R_intro"><h2>R for statistical computing</h2><figure style="width: 35%" class="right"><img src="images/r-usage-salary.png" style="width: 100%"/><figcaption>From O'Reilly's 2014 survey of <a href="http://www.oreilly.com/data/free/2014-data-science-salary-survey.csp">data science salaries</a>.</figcaption></figure><p>R is a leading tool in the industry for statistics and data science.<ul style="display: block"><li>is free and open-source</li><li>has an active community, which means you've got tons of analysis options.</li><li>is in demand!</li></ul></p></section><section data-transition-speed="fast"><h2>Installation</h2><p><ol><li>Install <a href="http://cran.cnr.berkeley.edu/">R</a></li><li>Install <a href="http://www.rstudio.com/products/rstudio/download/">R Studio</a></li></ol></p></section><section><h2>Project 1</h2><p><a href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%201/problem_statement_project_1.md">Project 1 </a><will>explores data from Washington D.C.'s Capital Bikeshare program. We want to identify the best predictors of a successful bike station. To complete the project you'll need to:</will></p><ul><li>formalize the business problem as a data mining process</li><li>compute the outcome (rentals per day)</li><li>use aggregation and summaries to create model inputs</li><li>analyze and interpret your results</li><li>write a brief report of your findings</li></ul></section><section data-transition-speed="fast"><h2>Project 1 data</h2><p>I've constructed a dataset from a couple different sources that includes<ul><li>usage data in 2012</li><li>daily weather in 2012</li><li>station details such as long/lat, and nearby amenities</li></ul></p></section><section><h2>Introductory Resources</h2><ol><li><a href="http://www.dcc.fc.up.pt/~ltorgo/DataMiningWithR/">Data Mining with R: Learning with Case Studies</a></li><li><a href="http://www.amazon.com/Ethics-Big-Data-Balancing-Innovation/dp/1449311792">Ethics of Big Data: Balancing Risk and Innovation</a></li><li><a href="https://www.coursera.org/course/rprog">Coursera course on R</a></li><li><a href="https://www.coursera.org/course/exdata">Exploratory Data Analysis from Coursera</a></li><li><a href="https://www.udacity.com/course/ud651">Exploratory Data Analysis from Udacity</a></li><li><a href="http://www.mmds.org/">Mining of Massive Datasets</a></li><li>The Elements of Statistical Learning. Trevor Hastie, Robert Tibshirani and Jerome Friedman. This is a classic text available <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">from their website</a></li><li><a href="http://onepager.togaware.com/">Hands-On Data Sciece with R</a></li><li><a href="http://datamining.togaware.com/">Data mining resources</a></li></ol></section><section id="regression" data-background="images/data_miner2.jpg" data-background-size="cover"><h2>Linear regression</h2><h4>BI Tech CP303 - Data Mining</h4><h4>Erin Shellman </h4><h4><a href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%201/linear-regression-in-R.Rmd">R Notebook</a></h4></section><section><h2>How many rentals per day?</h2><figure style="width: 40%" class="right"><img src="images/crosswalk_lm.png" style="width: 100%"/></figure><p>Predicting the expected number of bike rentals from a station per day is useful in lots of applications:<ul><li>Allocating bikes across stations</li><li>Budgeting for monthly bike maintenance</li><li>Picking the best locations for new stations</li></ul></p><p>We can use ordinary least squares (OLS) regression to estimate real-valued outcomes such as number of rentals per day.  </p></section><section data-transition-speed="fast"><h2>Why linear regression?</h2><p>Linear regression is a relatively simple method for predicting real-valued variables. It's great to have in your toolbelt due to the ease of interpretability and implementation, power, and flexibility.</p></section><section data-transition-speed="fast"><h2>Interpretability</h2><p>Regression coefficients have a straight-forward interpretation as the average change in the outcome for a unit change in an explanatory variable, all else equal. The coefficients also give a rough indication of what variables contribute most to the predicted value, which can be helpful in understanding causal mechanisms and for variable selection.</p></section><section data-transition-speed="fast"><h2>Ease of implementation</h2><p>Virtually all programming languages have implementations for linear regression, so they can be applied across many applications.</p></section><section data-transition-speed="fast"><h2>Powerful</h2><p>In prediction settings, linear models can outperform more complex nonlinear techniques, particularly when working with small training sets or sparse data. This is because regression is an averaging technique, making it less sensitive to missingness and extreme values.</p></section><section data-transition-speed="fast"><h2>Flexible</h2><p>Even in situations where linearity assumptions fail, regression can be applied on transformations of variables such as logarithms. </p></section><section data-transition-speed="fast"><h2>Mathematical foundations</h2><p>Let's walk through the foundations of regression with just one explanatory variable/<a href="http://en.wikipedia.org/wiki/Covariate">covariate</a>. Regression with just one covariate is called <a href="http://en.wikipedia.org/wiki/Simple_linear_regression">simple linear regression</a>. SLR has a simple framework and is fundamental to understanding more complex models.</p></section><section data-transition-speed="fast"><h2>Mathematical foundations</h2><figure style="width: 40%" class="right"><img src="images/crosswalk_lm.png" style="width: 100%"/></figure><p>SLR describes the relationship between an explanatory variable (or <i>independent</i> variable) and an outcome variable (sometimes called the <i>response</i> or <i>dependent variable</i>). It describes how the expected value of an outcome variable changes as the explanatory variable changes. <i>e.g.</i> How does the expected number of rentals change as the number of nearby crosswalks changes?</p></section><section data-transition-speed="fast"><h2>Mathematical foundations</h2><p>Given $n$ observations of $x$ and $y$,</p>$ (x_1, y_1), (x_2, y_2), \ldots , (x_n, y_n) $<p>the model for a simple linear regression is expressed as</p>$ y_i = \beta_{0} + \beta_{1}x_i + \epsilon_i $<p class="fragment current-visible">Hey, kinda looks like $y = mx + b$ where $\beta_0$ is the intercept and $\beta_1$ is the slope of the line.<figure style="width: 30%" class="right"><img src="images/Linear_Function_Graph.svg" style="width: 100%"/></figure></p></section><section data-transition-speed="fast"><h2>Which line is the best?</h2><p>How do we choose $\beta_0$ and $\beta_1$? With visual inspection alone, there seems to be several lines that could be drawn through the center of the points. We need an algorithm for standardizing and automating the process of finding the best one. </p><img src="images/three_lines.png" width="50%" height="50%" class="center"/></section><section data-transition-speed="fast"><h2>Errors</h2><img src="images/residuals.png" width="40%" height="40%" class="right"/><p>Data points will not fall exactly on the straight line. The distance of a point from the fitted line is called the error (or residual), $ \epsilon_i $. The errors are assumed to be independent and normally distributed with mean 0 and standard deviation $\sigma$.</p></section><section data-transition-speed="fast"><h2>Ordinary least squares ftw</h2><p>The method of <a href="http://en.wikipedia.org/wiki/Ordinary_least_squares">Least squares</a> selects the line that minimizes the sum of the squares of the errors. $\epsilon_i$ is the error of the prediction line for each observation.</p>$ \epsilon_i = (y_i - \beta_{0} - \beta_{1}x_i) $<p></p>$ \epsilon_i^2 = (y_i - \beta_{0} - \beta_{1}x_i)^2 $<p></p>Choose $\beta_0$ and $\beta_1$ so that $\sum\limits_{i=1}^n\epsilon_i^2$ is minimized.</section><section data-transition-speed="fast"><h2>Intuition</h2><p>Let's think about what the components of the regression equation represent to gain an intuition. Each $y_i$ is an observed value of the outcome variable. $\beta_{0} + \beta_{1}x_i$ is the estimated, or expected, $y_i$ value. So when we minimize the sum of squared errors we're making $(observed - expected)^2$ as close to 0 as possible.</p></section><section data-transition-speed="fast"><section data-transition-speed="fast"><h2>Assumptions</h2><p>To safely apply linear regression without fear of weird results, some assumptions must be met:<ol><li>Linearity between the outcome and covariates</li><li>Independence of the errors (no <a href="http://en.wikipedia.org/wiki/Autocorrelation">autocorrelation</a>)</li><li>Constant variance (<a href="http://en.wikipedia.org/wiki/Homoscedasticity">homoscedasticity</a>) of the errors</li><li>Normality of errors</li></ol></p><p>We might violate some assumptions and it's application-dependent whether we should be concerned about those violations.</p></section><section data-transition-speed="fast"><h2>Diagnostic plots</h2><p>In R, if you plot a linear model object, it'll give you a nice collection of diagnostic plots for assessing model assumptions.</p><pre><code data-trim="data-trim" contenteditable="contenteditable" class="r">model = lm(rentals ~ crossing, data = data)
par(mfrow = c(2, 2)) # make 4 plots in 2 rows, 2 cols
plot(model)</code></pre><img src="images/diagnostic_plot.png" width="50%" height="50%" class="center"/></section></section><section data-transition-speed="fast"><h2>Interpretation</h2><p>Regression equations have a nice interpretation. The intercept $\beta_0$ is the mean value of $y$ when $x = 0$. The slope of the regression line, $\beta_{1}$, is the change in the mean of y for each unit change in x.</p></section><section data-transition-speed="fast"><h2>R regression output </h2><p>The regression equation is </p>$ rentals = 27.53 + 0.49 * crosswalks $.<p>When there are no crosswalks the average number of bike rentals is about 28. For each additional crosswalk, we can expect an additional half rental. Or, for every two additional crosswalks, we can expect another rental.</p><pre><code data-trim="data-trim" contenteditable="contenteditable" class="r">model = lm(rentals ~ crossing, data = data)
summary(model)

Call:
lm(formula = rentals ~ crossing, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-51.815 -21.176  -8.394  13.765 126.969 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 27.53163    2.59354  10.615  < 2e-16 ***
crossing     0.49159    0.07031   6.992 4.92e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 29.54 on 183 degrees of freedom
Multiple R-squared:  0.2108,	Adjusted R-squared:  0.2065 
F-statistic: 48.88 on 1 and 183 DF,  p-value: 4.92e-11
</code></pre></section><section><h2>Assessing model quality</h2><p>We've described regression as method for numerical prediction, and used the regression of crosswalks on rentals as an example. Now we want to use that model to predict expected rentals. How do we know if our model is any good? </p></section><section data-transition-speed="fast"><section data-transition-speed="fast"><h2>Training and testing sets</h2><p>We measure prediction quality by simulating new data coming in. We do that by dividing the original data into a <i>training set</i> and a <i>testing set</i>. The <i>training set</i> is used to fit and tune the predictive model. Then, the <i>testing set</i> is used to validate the predictions. Generally construction of the train and test sets should be done completely at random.</p></section><section data-transition-speed="fast"><h2>Quick heuristics for study design</h2><p> <ol class="left">large sample size<li>60% train</li><li>20% test</li><li>20% validation</li></ol><ol class="center">medium sample size<li>60% train</li><li>40% test</li></ol><ol class="right">small sample size<li>Do cross validation</li></ol></p></section></section><section data-transition-speed="fast"><section data-transition-speed="fast"><h2>Measuring model quality for continuous outcomes<p>$\text{mean squared error (MSE)} = \frac{1}{n} \sum\limits_{i=1}^n (prediction_i - observed_i)^2$</p><p>$\text{root mean squared error (RMSE)} = \sqrt{\frac{1}{n} \sum\limits_{i=1}^n (prediction_i - observed_i)^2}$</p></h2></section><section data-transition-speed="fast"><h2>Which metric should I use?</h2><p>Just try them all, each metric has strengths and weaknesses.<ul><li>MSE and RMSE are sensitive to outliers.</li><li>Median absolute deviation is often more sensitive to outliers.</li></ul></p></section></section><section><h2>Linear Regression Resources</h2><ol><li><a href="https://class.coursera.org/predmachlearn-012">Practical Machine Learning</a> is a great Coursera course and is part of the data science specialization from Johns Hopkins.</li><li><a href="http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf">A Short Introduction to the caret Package</a></li><li><a href="http://www.jstatsoft.org/v28/i05/paper">Building Predictive Models in R Using the caret Package</a></li></ol></section><section id="selection" data-background="images/data_miner3.jpg" data-background-size="cover"><h2>Feature and Model Selection</h2><h4>BI Tech CP303 - Data Mining</h4><h4>Erin Shellman </h4><h4><a href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%201/linear-regression-in-R.Rmd">R Notebook</a></h4></section><section><section><h2>Simplify</h2><p>In the last section we built a regression model with 117 predictors. Unfortunately that's so many predictors that it's virtually impossible to interpret our results. Not only that, but to use a model that big for prediction would be a nightmare. It means that for every candidate station, we'd have to plug in 117 variables about the location and weather into the model to get the expected number of rentals. Yuck.</p><p>This week we'll explore automated methods for reducing model size while maintaining prediction accuracy.</p></section><section data-transition-speed="fast"><h2><a href="http://en.wikipedia.org/wiki/Curse_of_dimensionality">The curse of dimensionality</a></h2><img src="images/microarray.jpg" width="30%" height="30%" class="right"/><p>Linear regression works well when the number of observations, $n$, is larger than the number of predictors, $p$. In a data mining context we sometimes have data where $p >> n$. An example of this from the biomedical science is microarray technology. One tissue sample can produce over 50,000 observations making $p$ orders of magnitude larger than $n$. </p></section><section data-transition-speed="fast"><h2></h2><p>The "curse of dimensionality" is not a problem of high-dimensional data, but a joint problem of the data and the algorithm being applied. It arises when the algorithm does not scale well to high-dimensional data, typically due to needing an amount of time or memory that is exponential in the number of dimensions of the data.</p></section></section><section data-transition-speed="fast"><h2>Improve predictions</h2><figure style="width: 35%" class="right"><img src="images/bias_variance.png" style="width: 100%"/><figcaption><a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Understanding the Bias-Variance Tradeoff</a></figcaption></figure><p>Besides reducing the size of the unwieldy model, we also want to improve prediction accuracy. Accuracy can sometimes be improved by shrinking certain coefficients to zero. When we shrink coefficients we exchange a some bias to reduce the variance of the predicted values, and may improve the overall prediction accuracy.</p></section><section data-transition-speed="fast"><h2>What's a poor modeler to do?</h2><p>We'll talk about a couple methods for reducing our model size and improving prediction accuracy:</p><ul><li>combination of features </li><li>feature selection</li><li>shrinkage </li></ul></section><section data-transition-speed="fast"><h2>Combining features</h2><p>We'll talk about clustering methods that can be used to combine features later on, but for now we can combine some features with simple addition.</p><pre><code data-trim="data-trim" contenteditable="contenteditable" class="r">nightlife = bar + club + pub + nightclub 
</code></pre></section><section data-transition-speed="fast"><h2>Subset selection</h2><p>Subset selection methods use OLS regression, but limit the number of predictors $k$ allowed to remain in the model. They iteratively add dor remove predictors to the model until $k$ predictors are included. </p></section><section data-transition-speed="fast"><h2>Forward selection</h2><p>Fitting every possible subset of predictors will take eternity. Forward-stepwise selection (or just forward selection) starts with just the intercept and sequentially adds predictors to the model.  </p></section><section data-transition-speed="fast"><h2>Backward selection</h2><p>Backward-stepwise selection (or just backward selection) starts with the full model, and sequentially deletes predictors that have the least impact on the fit.</p></section><section data-transition-speed="fast"><h2>How to pick $k$?</h2><p>These selection methods all rely on the value of $k$, but how do we choose? Cross validation!</p></section><section data-transition-speed="fast"><h2>Use <a href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)">Cross validation</a> to determine the best parameter values</h2><figure style="width: 30%" class="right"><img src="images/crossvalidation.png" style="width: 100%"/><figcaption><a href="http://scott.fortmann-roe.com/docs/MeasuringError.html">Accurately Measuring Model Prediction Error</a></figcaption></figure><p>Cross-validation works by splitting the data up into sets of $n$ folds. Your model building and error estimation procedure is then repeated $n$ times. After completing a 5-fold cross-validation you'd have 5 error estimates and can average to obtain a robust estimate of the prediction error.</p></section><section data-transition-speed="fast"><h2>Shrinkage</h2><p>In our model of daily rentals, many predictors had quite small coefficients. For example, the coefficient on the number of stop signs, <i>stops</i>, is about -0.09. Since the outcome is rentals per day, a coeffcient this small means we'd need about 10 additional stop signs to reduce the expected rentals by 1. In other words, it's not contributing much to our predicted values.</p></section><section data-transition-speed="fast"><section data-transition-speed="fast"><h2>High variance</h2><p>You might have observed some coefficients with high variance. We have a lot of predictors, and when many are correlated, we violate the assumption of independence between predictors and the result is often high variance. A large positive coefficient on one predictor can be canceled by a similarly large negative coefficient on its correlate. Shrinking coefficients alleviates this problem, in part because some predictors will shrink to zero and drop out. </p></section><section data-transition-speed="fast"><h2>Lower variance, higher bias</h2><p>Applying the ridge regression penalty has the effect of shrinking the estimates toward zero which reduces variance but introduces bias.</p></section></section><section data-transition-speed="fast"><h2>Ridge regression</h2><p>Ridge regression <i>shrinks</i> regression coefficients by penalizing based on the coefficient size. In ordinary least squares, we picked $\beta$s that minimize our squared errors. Now we do that, but include a penalty parameter.</p><p>$ \epsilon_i^2 = \sum\limits_{i} (y_i - x^T_i\beta)^2 + \lambda\sum\limits_{j = 1}^p \beta_j^2 $ </p><p class="fragment current-visible">OLS + penalty</p></section><section data-transition-speed="fast"><section data-transition-speed="fast"><h2>Choosing $\lambda$</h2><figure style="width: 35%" class="right"><img src="images/ridge_coef.png" style="width: 100%"/><figcaption><a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning</a></figcaption></figure><p>The penalty parameter $\lambda$ controls the level of shrinking. $\lambda = 0$ will give you exactly the same coefficients as OLS. Beyond that, the choice of $\lambda$ will depend on the data and model, and we'll use cross validation to determine the best value.</p></section></section><section data-transition-speed="fast"><section data-transition-speed="fast"><h2>Lasso</h2><p>Like ridge regression, lasso is a shrinkage method with a major difference.</p><p>$\epsilon_i^2 = \sum\limits_{i} (y_i - x^T_i\beta)^2$ subject to $\sum\limits_{j = 1}^p |{\beta_j}| \leq t$   </p></section><section data-transition-speed="fast"><h2>Lasso</h2><p>Making t sufficiently small will cause some of the coefficients to be exactly zero, making the lasso method similar to a subset selection.</p></section></section><section data-transition-speed="fast"><h2>Choosing $t$</h2><figure style="width: 35%" class="right"><img src="images/lasso_coef.png" style="width: 100%"/><figcaption><a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning</a></figcaption></figure><p>If $t_0 \geq \sum\limits_{j = 1}^p |\beta_j|$, the lasso estimates and the OLS estimates are identical. If $t = t_0/2$ then the least squares coefficients are shrunk by about 50% on average. Once again we can pick the $t$ that gives the lowest test error, and we'll learn how to do that in <i>caret</i>.</p></section><section data-transition-speed="fast"><h2>Ethics</h2><p>Read <a href="http://www.theatlantic.com/features/archive/2014/07/why-poor-schools-cant-win-at-standardized-testing/374287/">this</a> article about how big data can be used to increase class divides.</p></section><section data-transition-speed="fast"><h2>Data are neutral, our conclusions are not.</h2><p>The Capitol Bikeshare dataset couldn't be more innocuous, but outside of privacy and security issues, it's almost never the data that create ethical dilemas, it's our application of the data and the conclusions we draw.<ul><li>What if your analysis showed that the best performing bike stations were in the wealthiest neighborhoods? </li><li>What if bikes are frequently used for commmuting in low-income areas?</li><li>What effect would removing stations in low-income neighborhoods have on their local economy?</li><li>Is it ok to charge more to rent in worse-performing areas to make up the difference?</li></ul></p></section><section data-transition-speed="fast"><h2>It's not always about the data you've got, it's also what you haven't.</h2><p>What variables are omitted from your data that could invalidate your findings?</p></section><section data-transition-speed="fast"><h2>Simpson's paradox</h2><p>A classic example of Simpson's paradox is from the University of California, Berkeley when they were sued for bias against women applicants to graduate programs. Admissions figures for fall 1973 showed that men were more likely to be admitted.</p><div><table class="table-hover"><thead><tr><th></th><th>Applicants</th><th>Admitted</th></tr></thead><tbody><tr><th>Men<td class="number">8442</td><td class="number">44%</td></th></tr><tr><th>Women<td class="number">4321</td><td class="number">35%</td></th></tr></tbody></table></div></section><section data-transition-speed="fast"><h2>Simpson's paradox</h2><p>However when examining the individual departments, the effect disappeared and even slightly reversed. It appears that these women applied to competitive departments with low admission rates more often than men who applied to less-competitive departments with higher admission rates.</p><div><table class="table-hover"><caption>Data from the 6 largest departments</caption><thead><tr><th rowspan="2">Department</th><th colspan="2">Men</th><th colspan="2">Women</th></tr><tr><th>Applicants</th><th>Admitted</th><th>Applicants</th><th>Admitted</th></tr></thead><tbody><tr><th>A<td class="number">825</td><td class="number">62%</td><td class="number">108</td><td class="number">82%</td></th></tr><tr><th>B<td class="number">560</td><td class="number">63%</td><td class="number">25</td><td class="number">68%</td></th></tr><tr><th>C<td class="number">325</td><td class="number">37%</td><td class="number">593</td><td class="number">34%</td></th></tr><tr><th>D<td class="number">417</td><td class="number">33%</td><td class="number">375</td><td class="number">35%</td></th></tr><tr><th>E<td class="number">191</td><td class="number">28%</td><td class="number">393</td><td class="number">24%</td></th></tr><tr><th>F<td class="number">272</td><td class="number">6%</td><td class="number">341</td><td class="number">7%</td></th></tr></tbody></table></div></section><section data-transition-speed="fast"><h2>Simpsons example</h2><p>Imagine you sent out a survey to homes nearby the bike stations and found that only 30% of people near poor-performing stations knew how to ride a bike? How would this change your conclusions and course of action?</p></section><section><h2>Feature Selection Resources</h2><ol><li><a href="http://topepo.github.io/caret/training.html">Modeling training and tuning with caret</a></li></ol></section><section id="logistic" data-background="images/data_miner4.jpg" data-background-size="cover"><h2>Classification with Logistic Regression </h2><h4>BI Tech CP303 - Data Mining</h4><h4>Erin Shellman </h4><h4><a href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%202/logistic-regression-in-R.Rmd">R Notebook</a></h4></section><section id="classification"><h2>Classification</h2><p>We use classification techniques when we're trying to assign something to a category.</p><ul><li>Spam or not spam</li><li>Diseased or not diseased</li><li>Fraud or not fraud</li></ul><p class="fragment current-visible">In our case, the outcome $y$ is either 0 or 1: $y \in{\{0, 1\}}$</p></section><section data-transition="linear" data-transition-speed="fast"><h2>Identifying spam</h2><p>We could build a spam classifier with a linear regression predicting whether a message is spam. The predictor is the number of occurrences of the character '$' in a message.</p><figure style="width: 45%; margin: 0 auto"><img src="images/is_it_spam.png" width="100%"/></figure></section><section data-transition="linear" data-transition-speed="fast"><h2>Identifying spam</h2><figure style="width: 45%" class="right"><img src="images/is_it_spam_2.png" width="100%"/></figure><p>Then to predict the class, create a threshold on $y$:<ul class="fragment"><li>if $y \geq 0.5$, spam</li><li>if $y < 0.5$, not spam</li></ul></p><p class="fragment current-visible">Messages with more than eight '$'s, will be labeled as spam.</p></section><section data-transition="linear" data-transition-speed="fast"><h2>Identifying spam</h2><p>What if an extreme value is observed later? The current rule isn't violated, but when we refit the model with the new data, the classifier begins to misclassify.</p><figure style="width: 45%; margin: 0 auto" class="left"><img src="images/is_it_spam_2.png" width="100%"/></figure><figure style="width: 45%; margin: 0 auto" class="right"><img src="images/is_it_spam_3.png" width="100%" class="fragment current-visible"/></figure></section><section data-transition-speed="fast"><section><h2>We need a new tool</h2><p>In a classification setting we know the outcome $y$ can only take the values $0$ or $1$, but in linear regression, $y$ could be much larger than $1$ and much smaller than $0$. Our current tool, linear regression, won't work well for classification problems. Instead we can use another type of regression called, <a href="http://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>.</p></section><section data-transition="linear" data-transition-speed="fast"><h2>History of GLM</h2><p>Articulated in Nelder and Wedderburn in 1972 paper in Journal of the Royal Statistical Society.</p><p>GLMs are composed of three parts:<ol><li>An exponential family model for the response</li><li>A systematic component via a linear predictor</li><li>A link function that connects the means of the response to the linear predictor</li></ol></p></section><section data-transition-speed="fast"><h2>Logistic regression GLM parts</h2><ol><li>Exponential family model:</li><li>Linear predictor: $ \eta_{i} = \sum_{k = 1}^{p} X_{ik}\beta_k $</li><li>Link: $ g(\mu) = \eta = \log{\mu \over 1-\mu} $</li></ol></section></section><section data-transition-speed="fast"><h2>Deriving logistic regression<p><b>Goal:</b> $0 \leq predictions \leq 1$</p><p class="fragment">$y = \beta_0 + \beta_{1}x$ &#10143 Simple Linear Regression</p><p class="fragment">How can we transform the right hand size to be between 0 and 1 like the outcome?<p class="fragment">$g(z) = \frac{1}{1+e^{-z}}$ &#10143 Logistic (or sigmoid) function</p></p><p class="fragment">$z = \beta_0 + \beta_{1}x$</p><p class="fragment">$y = \frac{1}{1 + e^{-(\beta_0 + \beta_{1}x)}}$ &#10143 Logistic Regression</p></h2></section><section data-transition-speed="fast"><h2>The logistic function $g(z) = \frac{1}{1+e^{-z}}$</h2><figure style="width: 50%; margin: 0 auto"><img src="images/logit_function.png" width="100%"/></figure></section><section data-transition-speed="fast"><h2>How to interpret predictions?</h2><p>Model predictoins represent the probability $y = 1$ given the data $x$.</p><p><i>e.g.</i> The probability of a message being spam, given the number of $\$'s$. If I get a value of 0.70 from my model, then $p(\text{message is spam}\mid 10 \text{\$'s}) = 0.70$.</p></section><section data-transition-speed="fast"><h2>How do we decide the class?</h2><p>The <i>decision boundary</i> separates the region where the model predicts $y = 1$ from the region where the model predicts $y = 0$.</p><ul><li>If $y \geq 0.5$, then the message is spam</li><li>If $y < 0.5$, then the message is not spam </li></ul><div><figure style="width: 45%" class="left"><img src="images/lower_decision_boundary.png" width="100%"/></figure><figure style="width: 45%" class="right"><img src="images/upper_decision_boundary.png" width="100%"/></figure></div></section><section data-transition-speed="fast"><h2>How do we decide the class?</h2><figure style="width: 35%; margin: 0 auto" class="right"><img src="images/decision_boundary.png" width="100%"/></figure><p>$p(\text{spam}) = -0.80 + 0.10*\text{number of \$s}$</p><p>predict y = 1, if: $-0.80 + 0.10x \geq 0$</p><p>$0.10x \geq 0.80$</p><p>$x \geq 8$</p><p>The line at $x = 8$ is called the <i>decision boundary</i>.</p></section><section data-transition-speed="fast"><h2>Bot <i>or</i> Not?</h2><p>Like many Internet giants Twitter makes money by selling ads, but they’ve got an insidious infestation eroding their advertising credibility: bots. More than 23 million of them. Twitter bots are automatons living in the Twittersphere and ranging wildly in capability. At their most complex they use speech patterns that can, at times, fool humans. </p><p>When advertisers pay for engagement, they aren’t interested in a four-hour flame war between a gamergate bot and a Kanye bot. When advertisers analyze social data they want to be sure the numbers are the result of human activity. </p></section><section data-transition-speed="fast"><h2>Bot <i>or</i> Not?</h2><p>For project 2, we'll use Twitter user data to build a classifier for identifying bots.</p></section><section data-transition-speed="fast" data-background="images/not_a_bot.png" data-background-size="cover"><h2 class="right">Not a bot...</h2></section><section data-transition-speed="fast" data-background="images/twitter_bot.png" data-background-size="cover"><h2 class="right">Spot a bot!</h2></section><section><h2>Logistic Regression Resources</h2><ol><li><a href="http://www.ats.ucla.edu/stat/r/dae/logit.htm">R Data Analysis Examples: Logit Regression</a></li><li><a href="http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm">FAQ: How do I interpret odds ratios in logistic regression?</a></li><li><a href="http://rpubs.com/ryankelly/ml_logistic">Classification tutorial in R</a> from Ryan Kelly</li></ol></section><section id="trees" data-background="images/data_miner5.jpg" data-background-size="cover"><h2>Classification with Trees</h2><h4>BI Tech CP303 - Data Mining</h4><h4>Erin Shellman </h4><h4><a href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%202/classification-trees-in-R.pdf">R Notebook</a></h4></section><section><h2>Classification with trees</h2><p>Tree-based methods are a conceptually simple but powerful method for classification (and regression). They work well in non-linear settings where regression might underperform.</p></section><section data-transition-speed="fast"><h2>Tree methods are intuitive </h2><figure style="width: 45%" class="right"><img src="images/decision_tree.gif" style="width: 100%"/><figcaption>Example tree from <a href="https://onlinecourses.science.psu.edu/stat557/node/83">here</a>.</figcaption></figure><p>An advantage of classification trees is their interpretability. For example in a clinical setting, they mirror the way that a clinician might think.</p></section><section data-transition-speed="fast"><h2>The construction of trees is iterative</h2><p>The basic algorithm can be summarized by the following steps: <ol><li>Start with all the input variables and repeat:</li><li>Find the variable and split that best "separates" the outcome</li><li>Divide the data into two groups on that split</li></ol></p><figure style="width: 35%; margin: 0 auto"><img src="images/tree_splitting.png" style="width: 100%"/><figcaption>Ron Meir's <a href="http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf">boosting tutorial</a>.</figcaption></figure></section><section data-transition-speed="fast"><h2>How to determine the splitting point?</h2><p>Pick the split points that yield the greatest 'purity' in the resulting subset. One measure that could be used is the overall misclassification error. </p><p>A misclassification error of 0 implies perfect purity and 0.5 implies an even mixture of both classes and no 'purity.'</p></section><section data-transition-speed="fast"><h2>Trees get big</h2><p>Tree methods can suffer from <a href="http://en.wikipedia.org/wiki/Overfitting">overfitting</a>, which occurs when our model is too tuned to the training data. When models are overfit they generalize poorly and exhibit high variance and poor prediction accuracy on new data.</p></section><section data-transition-speed="fast"><h2><a href="https://onlinecourses.science.psu.edu/stat557/node/93">Pruning</a></h2><figure style="width: 45%" class="right"><img src="images/tree-trimming-pruning-sl.jpg" style="width: 100%"/></figure><p>When we were doing regression, we started by fitting the <i>full model</i>. The full model had the lowest RMSE, but was also prohibitively complex. Tree methods can suffer a similar weakness in that resulting trees tend to be large and too complex. Just like regression, a smaller tree with fewer splits often leads to lower variance, easier interpretation and lower test errors, at the cost of a little bias.</p></section><section data-transition-speed="fast"><h2>Complexity penalty, $Cp$ </h2><p>The complexity penality ranges from 0 to 1 and penalizes the number of terminal nodes in the tree. A high value of $Cp$ corresponds to a decreased likelihood of a branch split and lower $Cp$ penalizes less dramatically. In <i>caret</i>, the <i>train()</i> function will identify an optimal $Cp$ for us.</p></section><section data-transition-speed="fast"><h2>The forest for the trees</h2><p>Single trees are simple to compute and pretty easy to understand, but they are susceptible to high variance when tested on new data. Methods such as boosting, bagging and random forests have been developed to overcome this problem.</p></section><section data-transition-speed="fast"><h2>Boosting</h2><p>Boosting works by combining the output of many weak classifiers to produce a powerful consensus tree. A weak classifier is one whose error rate is only slightly better than random guessing.</p></section><section data-transition-speed="fast"><h2><a href="http://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a></h2><figure style="width: 45%" class="right"><img src="images/adaboost.png" style="width: 100%"/><figcaption><a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning</a></figcaption></figure><p>Short for adaptive boosting, AdaBoost is one of the most popular boosting algorithms. AdaBoost is adaptive in the sense that subsequent weak learners are boosted in favor of the cases previously misclassified.</p></section><section data-transition-speed="fast"><h2>Bootstrap aggregating (bagging)</h2><figure style="width: 40%" class="right"><img src="images/bagging.png" style="width: 100%"/></figure><p>Bagging procedures work by randomly resampling data and refitting the model multiple times. The final model is determined by either averaging or taking a <i>majority vote</i> to form a single model. </p></section><section data-transition-speed="fast"><h2>Random Forest</h2><p>Random forest is a special type of bagging algorithm that resamples both the variables and the data used to fit the model. It's one of the best performing classifiers.</p></section><section><h2>Tree Resources</h2><ol><li><a href="https://rpubs.com/ryankelly/dtrees">Tree-based Methods</a>. A great tutorial from Ryan Kelly.</li><li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Decision Tree Learning</a></li><li><a href="https://class.coursera.org/predmachlearn-013">Practical Machine Learning</a></li><li><a href="https://onlinecourses.science.psu.edu/stat557/node/83">Penn State Data Mining course notes</a>. Great intro to decision trees.</li><li><a href="http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf">Boosting Tutorial</a> by Ron Meir.</li><li><a href="http://www.statsoft.com/Textbook/Classification-Trees">Classification Trees overview</a> from Statsoft</li></ol><figure style="width: 35%; margin: 0 auto"><img src="images/familyPlantDeciduous.jpg" style="width: 100%"/></figure></section><section id="rules" data-background="images/data_miner6.jpg" data-background-size="cover"><h2>Association rule mining</h2><h4>BI Tech CP303 - Data Mining</h4><h4>Erin Shellman </h4><h4><a href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%203/association-rules-in-R.Rmd">R Notebook</a></h4></section><section><h2>A new paradigm</h2><p>At this point you've built models to predict two outcome types: continuous and binary. Your predictivie models were based on training samples and predictive accuracy was assessed using a test set. This class of problems is called <i>supervised learning</i> because we observe the true value of the outcome, and those observations serve as a "teacher" for that model, providing it the correct answer and allowing us to compute measures of accuracy. <i>e.g.</i> RMSE and accuracy measurements.</p><p>For the remainder of the course, we'll discuss an <i>unsupervised learning</i> problem where we have no outcome variable at all.</p></section><section data-transition-speed="fast"><h2><a href="http://en.wikipedia.org/wiki/Unsupervised_learning">Unsupervised Learning</a></h2><p>In <i>unsupervised learning</i> applications, the goal is to find hidden structure in data when we do not observe an outcome. Since we have no observed outcome to compare our predictions with, we cannot compute an error to measure the predictive accuracy of our models. This distinguishes unsupervised from supervised learning.</p></section><section data-transition-speed="fast"><h2>Association Rule Mining</h2><p>Association rule mining is a popular tool for discovering patterns between variables in large transactional databases. The goal is to find recurring associations. </p><p>For example, Nordstom records transaction information from their stores and online. </p></section><section data-transition-speed="fast"><h2>Market basket analysis</h2><p>In a retail context, association rule mining is often referred to as "market basket" analysis. In this context the observations are sales transactions, such as those occurring at the checkout counter of a store.</p><p>The resulting association rules can be very valuable for use in cross-marketing in promotions, catalog design, consumer segmentation based and product recommendation.</p></section><section data-transition-speed="fast"><h2>Formalizing market basket analysis</h2><p>$I = \{item_1, item_2, \dots, item_n\}$ is a collection of binary attributes called <i>items</i>.</p><p>$D = \{t_1, t_2, \dots, t_m\}$ is a set of uniquely labeled transactions in the dataset.</p><p><i>Rules</i> take the form $X \Rightarrow Y$ where $X, Y \subseteq I$</p></section><section data-transition-speed="fast"><h2>Rules</h2><p>$X \Rightarrow Y$</p><p>The itemsets on the left-hand-side (LHS) are called the <i>antecedent</i> and the itemsets on the right-hand-side (RHS) are called the <i>consequent</i> of the rule.</p></section><section data-transition-speed="fast"><h2>Grocery store example</h2><p>The pool of items is $I = \{milk, bread, butter, beer\}$</p><div><p>The transaction dataset:</p><table class="table-hover"><thead><tr><th>Transaction Id</th><th>Itemsets</th></tr></thead><tbody><tr><td class="number">1</td><td class="number">milk, bread</td></tr><tr><td class="number">2</td><td class="number">bread, butter</td></tr><tr><td class="number">3</td><td class="number">beer</td></tr><tr><td class="number">4 </td><td class="number">milk, bread, butter</td></tr><tr><td class="number">5 </td><td class="number">bread, butter</td></tr></tbody></table></div></section><section data-transition-speed="fast"><h2>Filtering rules</h2><p>To identify interesting rules from all possible combinations, we need to construct a measure of significance. The most common constraints are imposed with the <i>support</i> and <i>confidence</i>. </p><p>Association rules are rules which surpass a user-specified minimum support and minimum confidence threshold.</p></section><section data-transition-speed="fast"><h2><a href="http://en.wikipedia.org/wiki/Association_rule_learning#Useful_Concepts">Support</a></h2><p>The <i>support</i> of an itemset is the proportion of transactions in the data which contain the itemset.</p><p>$support(A \Rightarrow B) = P(A \cup B)$</p></section><section data-transition-speed="fast"><h2>Support</h2><p>In the grocery example, the itemset $\{milk, bread\}$ occurs in 2 of 5 transactions, so it has a support of 2/5 = 0.4 = 40%. </p><p>What is the support for $\{bread, butter\}$?</p></section><section data-transition-speed="fast"><h2>Confidence</h2><p>The <i>confidence</i> of a rule is an estimate of the conditional probability $p(B|A)$, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.</p><p>$confidence(A \Rightarrow B) = \frac{support(A \cup B)}{support(A)}$</p></section><section data-transition-speed="fast"><h2>Confidence</h2><p>The rule $\{milk, bread\} \Rightarrow \{butter\}$ has a confidence of 0.2/0.4 = 0.5, meaning that for 50% of the transactions containing $\{milk, bread\}$ the rule holds.         </p></section><section data-transition-speed="fast"><h2>Constraining rules</h2><p>When association rules are constructed, they must satisfy both minimal support and confidence thresholds, however even with these requirements we're often left with a long list of candidate rules. We can further reduce the pool of rules using the <i>lift</i> metric.</p></section><section data-transition-speed="fast"><h2>Lift</h2><p>$lift(A \Rightarrow B) = \frac{support(A \cup B)}{support(A) \times support(B)}$</p><p>Lift is the deviation in the support of the whole rule from the support expected under independence given the supports of the LHS and the RHS. Larger lift indicates stronger associations.</p></section><section data-transition-speed="fast"><h2>Interpreting lift</h2><p>A lift of 1 indicates that the association between the LHS and RHS is exactly what we'd expect if there is no relationship between the itemsets on either side, <i>i.e.</i> the itemsets are independent. Imagine you're at the grocery story buying your bread, butter and milk, and you remember that you need vacuum cleaner bags.</p><p>Lift values greater than 1 indicate that there's some depedence between the itemsets. For example chips and salsa, or beer and pretzels.</p></section><section data-transition-speed="fast"><h2>The <a href="http://en.wikipedia.org/wiki/Apriori_algorithm">apriori algorithm</a></h2><figure style="width: 45%" class="right"><img src="images/apriori.png" style="width: 100%"/></figure><p>The <i>apriori</i> algorithm is an iterative, breadth-first approach to finding frequent itemsets and deriving association rules from them.</p></section><section data-transition-speed="fast"><h2>Data</h2><p>In project 3 we'll use a dataset created from crawling Amazon.com. The crawler collected sets of recommended products from scraping the <i>Customers who bought this item also bought</i> feature on the website. The data were collected in March 2003.</p></section><section><h2>Association Rules Resources</h2><ol><li><a href="http://www.rdatamining.com/examples/association-rules">Association Rule Example</a></li><li><a href="http://www.rdatamining.com/docs/association-rule-mining-with-r">Association Rule Mining in R</a></li><li><a href="http://www.salemmarafi.com/code/market-basket-analysis-with-r/">Market Basket Analysis with R</a></li></ol></section><section id="clustering" data-background="images/data_miner7.jpg" data-background-size="cover"><h2>Clustering</h2><h4>BI Tech CP303 - Data Mining</h4><h4>Erin Shellman </h4><h4><a href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%203/clustering-in-R.Rmd">R Notebook</a></h4></section><section><h2></h2><p>Last time, we used 'market baskets' from Amazon.com to generate association rules between products based on purchasing behavior. We can then use the resulting rules to make product recommendations. Now, suppose we didn't have any knowledge of purchasing behavior and only knew features of the product themselves <i>e.g.</i> from the product catalog. Can we use that information to create a recommender?</p></section><section data-transition-speed="fast"><h2>Content-based recommendations</h2><figure style="width: 45%" class="right"><img src="images/pandora.png" style="width: 100%"/><figcaption>Pandora uses a <a href="http://en.wikipedia.org/wiki/Music_Genome_Project">music genome</a> with over 450 attritubes to measure song similarity. </figcaption></figure><p>Recommendations based on the attributes, or features, of products are called <i>content-based</i> (or product-based) recommendations. The assumption is that customers are inclined to favor products that are similar to ones they already like.</p></section><section data-transition-speed="fast"><h2>Clustering</h2><p>The goal of cluster analysis (or segmentation) is to group observations into subsets (clusters) such that objects within a cluster are more closely related to one another than observations in different clusters. The clustering is accomplished by grouping observations according to a <a href="http://en.wikipedia.org/wiki/Distance">distance metric</a>.</p></section><section data-transition-speed="fast"><h2><a href="http://en.wikipedia.org/wiki/K-means_clustering">K-means Clustering</a></h2><p>One of the most popular clustering algorithms, the goal of K-means clustering is to group n observations into k clusters. K-means has essentially two steps: </p><ol><li>Cluster assignment step - assign all observations to one fo the centroids based on which is closest.</li><li>Centroid shift step - move the centroids to the average value of the points within the cluster.</li></ol></section><section data-transition-speed="fast"><h2>K-means Algorithm</h2><figure style="width: 25%" class="left"><img src="images/K_Means_Example_Step_1.svg" style="width: 100%"/><figcaption>1. Randomly initialize K = 3 cluster centroids (red, green, and blue). </figcaption></figure><figure style="width: 25%" class="left"><img src="images/K_Means_Example_Step_2.svg" style="width: 100%"/><figcaption>2. Assign each observation to a cluster based on which centroid they're nearest.</figcaption></figure><figure style="width: 25%" class="left"><img src="images/K_Means_Example_Step_3.svg" style="width: 100%"/><figcaption>3. Find the mean of the clusters and move the K centriods to those means.</figcaption></figure><figure style="width: 25%" class="left"><img src="images/K_Means_Example_Step_4.svg" style="width: 100%"/><figcaption>4. Repeat steps 2 and 3 until the centroids stop moving.</figcaption></figure></section><section data-transition-speed="fast"><h2>Selecting K</h2><p>The number of centroids, K, is a required input for the K-mean algorithm. The choice of the size of K depends on the goal. For segmentation applications, K is typically defined by the context of the problem. For example, a salesteam may have K members, and the goal is to assign each salesperson a set of leads such that the customers assigned to each are similar as possible. But often K is unknown.</p></section><section data-transition-speed="fast"><h2>Selecting K</h2><p>Most often, K is determined manually by visualizing data. This is a common problem in unsupervised learning.</p><p>Check out this <a href="https://www.youtube.com/watch?v=BVFG7fd1H30">cool video</a> of K-means simulations with varying $K$s and $N$s.</p></section><section data-transition-speed="fast"><h2><a href="http://en.wikipedia.org/wiki/Hierarchical_clustering">Hierarchical Clustering</a></h2><p>The results of K-means depend on the value of K and starting position of the centroids, but another clustering method, hierarchical clustering, does not require us to specificy the number of clusters. Instead we specify a distance metric between observations. </p><p>This method produces hierarchical representations in which the clusters at each level of the hierarchy are created by merging clustering at the next lower level. At the lowest level each cluster contains a single observation. At the highest level there is only one cluster containing all the data.</p></section><section data-transition-speed="fast"><h2>Methods of hierarchical clustering</h2><figure style="width: 35%" class="right"><img src="images/hier_cluster.jpg" style="width: 100%"/></figure><p>There are two main types of hierarchical clustering:</p><ol><li>Agglomerative - a 'bottom up' approach in which each observation is initially its own cluster and pairs of observations are iteratively combined.</li><li>Divisive - a 'top down' approach in which all observations start in a single cluster and recursively split.</li></ol></section><section data-transition-speed="fast"><h2>Distance</h2><p>To decide which clusters to combine or divide, a distance metric (also called a dissimilarity) between observations is required. How can we measure the distance between two observations?</p></section><section data-transition-speed="fast"><h2>Euclidean Distance</h2><figure style="width: 45%" class="right"><img src="images/euclidean_dist.jpg" style="width: 100%"/><figcaption>Look familiar? It's an application of the Pythagorean theorem!</figcaption></figure><p>You already know one distance metric: <a href="http://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a>. Distances are often referred to as <i>similarities</i> or <i>dissimilarities</i>.</p></section><section data-transition-speed="fast"><h2>Dendrograms</h2><p>A common tool for representing hierarchical clusters is the dendrogram. The distance between observations is represented on a dendrogram by the length of the edge.</p><figure style="width: 45%" class="left"><img src="images/circular_dendrogram.png" style="width: 100%"/><figcaption>Circular dendrogram</figcaption></figure><figure style="width: 45%" class="left"><img src="images/heatmap_dendrogram.png" style="width: 100%"/><figcaption>Heatmap with row and column dendrograms</figcaption></figure></section><section data-transition-speed="fast"><h2>Clustering Resources</h2><ol><li>Nice presentation by <a href="http://www.slideshare.net/PhamCuong/clustering-technique-for-collaborative-filtering-recommendation-and-application-to-venue-recommendation">Pham Cuong</a> about clustering techniques for recommendations.</li><li><a href="http://manuals.bioinformatics.ucr.edu/home/R_BioCondManual#R_clustering">Clustering and Data Mining in R</a></li><li><a href="http://www.rdatamining.com/docs/data-clustering-with-r">Data Clustering with R</a>, from RDataMining.com.</li></ol></section><section id="share" data-background="images/data_miner9.jpg" data-background-size="cover"><h2>Sharing is caring!</h2><h4>BI Tech CP303 - Data Mining</h4><h4>Erin Shellman </h4><h4><a href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/R-markdown-demo.Rmd">R Notebook</a></h4></section><section data-transition-speed="fast"><h2>You've come a long way, baby</h2><ol>What you've learned<li>Prediction of a continuous outcome<ul><li>Linear regression</li><li>Ridge regression</li><li>Lasso regression</li></ul></li><li>Prediction of a binary outcome<ul><li>Logistic regression</li><li>Decision trees<ul><li>Bagging</li><li>Boosting</li><li>Random Forest</li></ul></li></ul></li><li>Unsupervised learning<ul><li>Association rule mining</li><li>K-means clustering</li><li>Hierarchical clustering</li></ul></li></ol></section><section data-transition-speed="fast" data-transition="linear" data-background="images/claps.gif" data-background-size="200px" data-background-repeat="repeat"><h1 style="color:#7FFF00" class="huge boxed">GREAT JOB!!!</h1></section><section data-transition-speed="fast"><h2>And now... share your work!</h2><p>You've learned a ton about data mining, problem solving and writing technical reports. To make the most of your efforts, let's create digital portfolios of work with <a href="http://rpubs.com/">RPubs</a> and Markdown.</p></section><section data-transition-speed="fast"><h2><a href="http://daringfireball.net/projects/markdown/">Markdown</a></h2><p>Markdown is a text-to-HTML conversion tool. It allows you to write your text using an easy-to-read, easy-to-write plain text format, then convert it to structurally valid HTML. Markdown is</p><ol><li>a plain text formatting syntax and</li><li>a software tool, written in Perl, that converts the plain text formatting to HTML</li></ol></section><section data-transition-speed="fast"><h2>An analysis and communication tool combined</h2><p>Markdown gives us a powerful communication tool that can be embedded with R code so that we can document analyses as we go. This is a powerful workflow because it keeps communication and clarity of thought top of mind. Ultimately our goal is to report and communicate findings, why not do that with a single tool?</p></section><section data-transition-speed="fast"><h2>Reproducibility </h2><p>Tools like Excel can be great for doing quick explorations, but as your analyses become complex, your work becomes increasingly unreproducable. That lack of reproducibility isn't just a pain for the consumers of your analyses, but it's a huge pain for you. </p><p class="fragment">"Prior to shifting to R, my basic approach was to enter data in Excel, import the data into Systat, and then use the command line to do a bunch of manipulations in there. I generally viewed those as one-off manipulations (e.g., calculating log density), and, while I could have saved a command file for those things, I didn’t. This meant that, if I discovered an error in the original Excel file, I needed to redo all that manually." <a href="https://dynamicecology.wordpress.com/2015/02/18/the-biggest-benefit-of-my-shift-to-r-reproducibility/">Meghan Duffy</a></p></section><section data-transition-speed="fast"><h2>Interactivity</h2><p>Another advantage of using a combination of R and Markdown as both your analysis and communication tool is that it enables interactivity, which can help engage your audience and allow them to explore the data themselves. As an example, check out this <a href="https://erinshellman.shinyapps.io/paid-leave-calculator/">benefits calculator</a> that a co-worker and I built to help companies understand the pros and cons of offering paid leave to employees.</p></section><section data-transition-speed="fast"><h2><a href="http://rmarkdown.rstudio.com/">R Markdown</a></h2><p>R Markdown is a custom flavor of Markdown and an authoring format that enables easy creation of dynamic documents, presentations, and reports from R. It combines the core syntax of Markdown with embedded,runnable R code. R Markdown documents are fully reproducible, so that they can be automatically regenerated whenever underlying R code or data changes.</p></section><section data-transition-speed="fast"><h2><a href="http://yihui.name/knitr/">KnitR</a></h2><p>The <i>knitr</i> package was designed to be a transparent engine for dynamic report generation with R. KnitR is the library that actually compiles the Markdown.</p><p class="fragment">"When I was first starting out, I’d create a bunch of figures and tables and email them to my collaborator with a description of the findings in the body of the email. That was cumbersome for me and for the collaborator. ('Which figure are we talking about, again?')" <a href="http://kbroman.org/knitr_knutshell/pages/overview.html">Karl Broman</a></p></section><section data-transition-speed="fast"><h2>A better way to communicate</h2><p>"...Now, I deliver my informal reports to collaborators as html documents that can be viewed in a browser. A big advantage to this is that I don’t have to worry about page breaks. For example, I can have very tall figures, with say 30 panels. That makes it easy to show the results in detail, and you don’t have to worry about how to get figures to fit nicely into a page." <a href="http://kbroman.org/knitr_knutshell/pages/overview.html">Karl Broman</a></p></section><section data-transition-speed="fast"><h2>Make a new R Markdown doc</h2><figure style="width: 45%" class="right"><img src="images/new_markdown.png" style="width: 100%"/><figcaption>Open a new Markdown document</figcaption></figure><p>Once you select a new Markdown document you can choose from documents, presentations, Shiny application or from a template.</p></section><section data-transition-speed="fast"><h2>Denoting code </h2><p>Backticks ` and ```{r chunk_name, options}. If you want to put code snippets in line with your text just wrap them in `the ticks`. To make bigger code chunks, then do this:</p><pre><code data-trim="data-trim" contenteditable="contenteditable" class="r">```{r linear regression}
require(ggplot2)
data(diamonds)
model = lm(carat ~ depth, data = diamonds)
```</code></pre></section><section data-transition-speed="fast"><h2>Output your report to a file</h2><figure style="width: 45%" class="right"><img src="images/save_markdown.png" style="width: 100%"/><figcaption>Export your Markdown to a format of your choice</figcaption></figure><p>You can save as HTML, PDF or a Word Document.</p></section><section data-transition-speed="fast"><h2>Or, share your notebooks online!</h2><p><a href="http://rpubs.com/">RPubs</a> is a free service from RStudio where you can upload your R Markdown files and view them online. </p></section><section data-transition-speed="fast"><h2>Sharing Resources</h2><ol><li><a href="https://dynamicecology.wordpress.com/2015/02/18/the-biggest-benefit-of-my-shift-to-r-reproducibility/">The biggest benefit of my switch to R? Reproducibility</a></li><li><a href="http://daringfireball.net/projects/markdown/dingus">Online Markdown editor</a></li><li><a href="http://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf">R Markdown Cheatsheet</a></li><li><a href="http://www.noamross.net/blog/2013/1/7/collaborating-with-r.html">A guide to tools for collaboration with R</a> by Noam Ross</li></ol></section><section id="acknowledgements"><h2>Thanks buddies!</h2><p>Creating this course wasn't easy and I couldn't have done it alone. Huge thanks to my TA Ryan Brush who encouraged and recommended me to teach this class. Big ups to <a href="http://brianbeck.com/">Brian Beck</a> who styled the lecture notes. </p><p>Thank you to the <a href="https://www.flickr.com/photos/boston_public_library/sets/72157627774101436">Boston Public Library</a> for posting all the awesome old coal mining postcards on Flickr!</p></section><section id="author"><h2>But who <i>is</i> <a href="http://www.erinshellman.com/">Erin Shellman</a>?  </h2><figure style="width: 30%" class="right"><img src="images/shellman.jpg" style="width: 100%"/></figure><p>I’m a statistician, programmer, and senior data scientist at <a href="http://zymergen.com/">Zymergen</a>. I’ve done research and data science in a broad range of industries including retail, cloud computing, and biotechnology. Along the way, I built product recommendations, web scrapers, interactive visualizations, and analyzed terabytes of data. I’m passionate about technology education and teach data mining at the University of Washington’s school for Professional and Continuing Education and Python programming in an after-school <a href="http://girlswhocode.com/">Girls Who Code</a> club. I also co-organize the Seattle chapter of <a href="http://www.meetup.com/Seattle-PyLadies/">PyLadies</a>, an international mentorship group for women who program in Python.</p></section></div></div><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.0.0-beta1/jquery.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/foundation/6.2.0/foundation.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/headjs/0.96/head.min.js"></script><script src="js/reveal.js"></script><script>Reveal.initialize({
  //width: 1200,
  //height: 900,
  width: 960,
  height: 700,
  
  margin: 0.1,
  
  controls: true,
  progress: true,
  history: true,
  center: true,
  theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
  transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none
  math: {
    mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
    //mathjax: 'js/MathJax.js',
    config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
  }, 
  dependencies: [
    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
    { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
    { src: 'plugin/math/math.js', async: true },
    { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
  ]
});</script></body></html>