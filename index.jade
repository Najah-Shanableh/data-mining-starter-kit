//doctype html
html
html(lang="en")
  head
    title BI TECH CP303 - Data Mining

    meta(charset="UTF-8")
    meta(name="fragment", content="!")
    meta(name="viewport", content="width=device-width, initial-scale=1, maximum-scale=1")

    link(rel="stylesheet", href="css/foundation.min.css")
    link(rel="stylesheet", href="css/reveal.min.css")
    // Theme for syntax highlighting
    link(rel="stylesheet", href="lib/css/zenburn.css")
    link(rel="stylesheet", href="css/theme/data-mining.css")

    // For exporting to pdf
    script(type='text/javascript').
      if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
      }

  body

    header.full-width.header-area
      div.row
        div.large-12.columns

    #main-nav
    div.contain-to-grid.navigation-area
      div.row
        div.large-12.columns
          nav.top-bar
            ul.title-area
              img(src="images/uw_logo.svg" id="logo-image")
              li.toggle-topbar.menu-icon
                a(href='#') nav            
                
            section.top-bar-section
              ul.right
                li.has-dropdown.not-click
                  a.active(href='#') Topic
                  ul.dropdown
                    li.active
                      a.active(href='#/week1') Intro to data mining and R
                    li.active
                      a.active(href='#/week2') Linear regression
                    li.active
                      a.active(href='#/week4') Logistic regression
                    li.active
                      a.active(href='#/week5') Classification Trees 
                    li.active
                      a.active(href='#/week6') Association Rules
                    li.active
                      a.active(href='#/week7') Clustering
                    li.active
                      a.active(href='#/week8') Forecasting
                    li.active
                      a.active(href='#/week10') Markdown + Git
                li.has-dropdown.not-click
                  a.active(href='#') About 
                  ul.dropdown
                    li.active
                      a.active(href='#/how_to') how to use these slides 
                    li.active
                      a.active(href='#/textbook') textbook
                    li.active
                      a.active(href='#/author') the author
                    li.active
                      a.active(href='#/acknowledgements') acknowledgements
                li.active.not-click
                  a.active(href='https://github.com/erinshellman/data-mining-starter-kit/issues') Report a bug

    .reveal
      .slides
        section.title-slide
          h2 BI Tech CP303 - Data Mining
          h4 Erin Shellman - 
             a.active(href="mailto:shellman@uw.edu") shellman@uw.edu
          h5 <b>TA</b>: Ryan Brush - 
             a.active(href="mailto:rtbrush@uw.edu") rtbrush@uw.edu
          p Welcome! These are lecture notes that accompany an applied course on data mining. Read the #[a(href='https://github.com/erinshellman/BI-TECH-CP303/blob/master/README.md') syllabus] to find details about assignments and deadlines.

        section#textbook(data-transition-speed="fast")
          h2 Textbooks
          p There is no book required for this course, however there are two great texts available for free online.
          .flex-row
            a.left(href="http://www.mmds.org/")
              img(src="images/mmds.jpg" style="height: 40%")
            a.right(href="http://www.statsoft.com/Textbook/Data-Mining-Techniques")
              img(src="images/hill_lewicki.jpg" style="height: 40%")
            
        section(data-transition-speed="fast")
          h2 Course Philosophy
          p Data mining is an applied skill and this course focuses on applications. Each class will be a blend of concepts and demonstrations of the concepts using the programming language R. There will be four projects, each consisting of a data mining analysis to solve a business problem and a brief written report of your findings.

          p By the end of this course, you will be able to approach unfamiliar data and quickly find and communicate results.

        section(data-transition-speed="fast")
          section(data-transition-speed="fast")
            h2 Navigating the notes
            p The notes progress from left to right, and for those interested in exploring more about a topic sometimes there's bonus material in the up, down direction.

          section(data-transition-speed="fast")
            h2 Just like this!

        section(data-transition-speed="fast")
          h2 Traverse the topics 
          p You can also dive into topics using the navigation bar at the top.

        section(data-transition-speed="fast")
          h2 Bookmark and check back often!
          p Data technology is a rapidly growing and changing industry. I chose #[a(href="http://lab.hakim.se/reveal-js/#/") this format] for course notes so that they could be easily maintained and up-to-date. As you apply these skills outside of class, I hope you can use these notes as a reference.

        section#how_to(data-transition-speed="fast")
          h2 Notes improved with the support of viewers like you!
          p These slides are a work in progress and I need your help to make them rule. If you see an error or think something is unclear or omitted, please #[a(href='https://github.com/erinshellman/data-mining-starter-kit/issues') report it!]
          p I'll do my best to keep the slides as updated and accurate as possible.

        section(data-transition-speed="fast")
          h2 What are your professional goals?
          blockquote R programming and use familiarity with Statistics so I can apply it on other platforms not covered in class, clarity of concepts and familiarity with tool so I have a basis we can build upon.  

        section(data-transition-speed="fast")
          h2 What are your professional goals?
          blockquote It would be awesome if we are able to publish our work as part of a portfolio.  

        section#week1.title-slide(data-background="images/data_miner1.jpg" data-background-size="100%")
          h2 Week 1: Introduction to Data Mining with R
          h4 BI Tech CP303 - Data Mining
          h4 
            a(href='https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/intro-R/introduction-to-r.Rmd') R Notebook

        section(data-transition-speed="fast") 
          section(data-background="images/data-explosion.png" data-background-size="100%")
            //h1 We're creating data faster than we can understand it.

          section(data-transition-speed="fast") 
            h2 We are inundated with data
            p Governments, corporations, scientists, and individual consumers are creating and collecting more data than ever before. Using clickstream technology, the modern advertiser can follow would-be customers as they browse online. These marketers are searching for clues about your preferences and shopping habits, evidence of your family composition, and even upcoming events in your life, all for the purposes of "personalization." On an individual level, Fitbits, Fuel bands, and Jawbone all allow us to monitor ourselves 24 hours a day, while 23andme gives us data about our genetic make-up. Not only do <i>we</i> consume personal data, but we <i>share</i> it with others on social media. Similarly the #[a(href="https://ropensci.org/blog/2014/11/10/open-data-growth/") biological] and #[a(href="http://earthobservatory.nasa.gov/Features/LandsatBigData/") earth] sciences are exploding with data as the cost of collecting it has plummetted. And the government, of course, wants all the data. 

          section(data-transition-speed="fast") 
            h2 We are inundated with data
            p But that's just it, we're <i>collecting</i> data in faster than ever in history, but our growth of <i>understanding</i> of these data has not kept pace. 

        section(data-transition-speed="fast") 
          section(data-background="images/data-variety.png" data-background-size="contain")
          //h1 Data are more heterogenous than ever.

          section(data-transition-speed="fast") 
            h2 Data representation and storage is more heterogeneous than ever
            p Not only are we generating more data faster, we're developing more ways to represent and store data. Marketing personalization combines disperate data about a single person or household: demographics, transactional histories, social media activities and clickstream. Personalizatin efforts require the ability to ingest and combine data from many sources across many formats. Graph databases like #[a(href="http://thinkaurelius.github.io/titan/") Titan] or #[a(href="http://neo4j.com/") neo4j] represent complex interactions as network graphs and analysts query the network with traversal patterns. Algorithms like mapreduce on the other hand, revolutionized the way we query #[a(href="http://en.wikipedia.org/wiki/Unstructured_data") unstructured data], like text and weblogs.

        section(data-transition-speed="fast")
          h2 We need new tools
          p In the context of big, heterogeneous data, the number of candidate features is too large to explore with traditional analysis techniques. Further, the volume of data create new challenges that analyists historically haven't dealt with such as
          ul
            li computational efficiency, <i>i.e.</i> speed
            li memory use and allocation, <i>i.e.</i> computing resources
            li data pipelines and integrity
            li the ethical consequences of findings

        section#what_is(data-transition-speed="fast") 
          section(data-transition-speed="fast") 
            h2 Data mining is a process of knowledge discovery 
            img.right(src="images/data_mining_process.png" style="height: 70%")
            p Data Mining is the process of identifying patterns in large volumes of data <u>for the purposes of prediction</u>. Realistically, data cleaning and exploratory analysis will occupy the vast majority of your time with the rest left over for model discovery. Model deployment is optional and depends on the result of the first three steps. After completing the analysis you might find that long-term adoption of your model isn't worthwhile.

          section(data-transition-speed="fast") 
            h2 Additional definitions
            blockquote Data mining is the extraction of implicit, previously unknown, and potentially useful information from data. #[a(href='http://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569') Witten, Frank and Hall]
            blockquote The most commonly accepted definition of “data mining” is the discovery of “models” for data." Where 'model' could mean a statistical model, and machine learning model or an algorithm like mapreduce. #[a(href='http://www.mmds.org/') Leskovec, Rajaraman, Ullman]
            blockquote Data Mining is an analytic process designed to explore data in search of consistent patterns and/or systematic relationships between variables, and then to validate the findings by applying the detected patterns. #[a(href="http://www.statsoft.com/Textbook/Data-Mining-Techniques") Hill & Lewicki]

        section(data-transition-speed="fast") 
          section(data-transition-speed="fast") 
            h2 How do statistics and machine learning relate?
            p Statistics and machine learning are tools that we employ in our data mining process. Generally the field of statistics is concerned with the analysis and interpretation of data, while the field of machine learning is primarily focused on prediction. Machine learning is less concerned with interpretation and validity of statistical assumptions (to the extent that they don't negatively influence the model's ability to predict).

          section(data-transition-speed="fast")
            p To learn more about the different paradigms of inferential statistics and machine learning I highly recommend #[a(href="http://faculty.smu.edu/tfomby/eco5385/lecture/Breiman%27s%20Two%20Cultures%20paper.pdf") Statistical Modeling: The Two Cultures] by Leo Breiman.

        section(data-transition-speed="fast")
          h2 Machine Learning
          p #[a(href="http://en.wikipedia.org/wiki/Machine_learning") Machine learning] is a discipline originating in artificial intelligence that creates and investigates algorithms that help machines <i>learn</i> from data. A machine can be said to learn if its behavior changes such that it performs better in the future.
          p Applications of machine learning are often in automated systems spam detectors, or recommendation engines.

        section(data-transition-speed="fast")
          p You'll find that many of the techniques you've used in an analysis setting are identical to those used in machine learning, for example, linear and logistic regression. The key difference is that in machine learning, we'll judge our model quality by assessing its ability to predict on hold-out data, rather than through goodness-of-fit metrics like $ R^2 $ or AIC.

        section(data-transition-speed="fast")
          h2 Lets look at some examples
          p Analysis problems generally fall into one of four classes of problem and we're going to try to get some hands-on experience with all four:
            ol
              li numerical prediction 
              li classification (or #[a(href="http://en.wikipedia.org/wiki/Supervised_learning") supervised learning])
              li clustering and #[a(href="http://en.wikipedia.org/wiki/Unsupervised_learning") unsupervised learning]
              li time-series

        section(data-transition-speed="fast")
          h2 Capital Bikeshare 
          p In project 1 you'll analyze data from Washington D.C.'s bikeshare program to predict locations of successful bike stations. An model for this application could be to predict the number of bike rentals per hour as a function of the number of restaurants nearby.
          img.center(src="images/crosswalk_lm.png" style="height: 45%")

        section(data-transition-speed="fast")
          h2 Bot or not?
          p In project 2, you'll build a classifier to distinguish Twitter bots from humans.

        section(data-transition-speed="fast")
          h2 Pandora 2.0
          p In project 3, you'll use association rule mining and clustering to build a song recommender using a subset of the Million Song Dataset.

        section(data-transition-speed="fast")
          h2 Predicting network traffic
          p Finally in project 4 you'll forecast network traffic by analyzing historical data.

        section(data-transition-speed="fast")
          h3 Limitations of data mining
          p Like any analytical technique, and arguably more so than traditional analysis, data mining has limitations:
            ul 
              li many patterns are uninteresting
              li patterns can be coincidental or a result of sampling bias
              li data are almost always untidy or missing
          p How do these limitations influence the interpretation and application of the results of a data mining process? What are our standards for belief?

        section
          section
            h2 Ethical data mining
            p As a result of the volume and speed at which businesses can collect data, conversations about the ethical use of these data are more important than ever. We're used to reading headlines about ethical violations in big data, but often these incidents can be anticipated and avoided.

          section(data-transition-speed="fast")
            p Here's a fairly horrifying list of articles about data mining and potential ethical violoations.
            ul
              li #[a(href="http://www.nytimes.com/roomfordebate/2014/08/06/is-big-data-spreading-inequality/losing-out-on-employment-because-of-big-data-mining") Losing Out on Employment Because of Big Data Mining]
              li #[a(href="http://arstechnica.com/gadgets/2012/05/on-facebook-deleting-an-app-doesnt-delete-your-data-from-their-system/") On Facebook, deleting an app doesn’t delete your data from their system]
          
        section(data-transition-speed="fast")
          h2 Ethics case study: Target
          p Take 20 minutes and read the following piece from the New York Times Magazine which describes how Target used transactional data to target expectant mothers for marketing campaigns: #[a(href="http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html") How Companies Learn Your Secrets]

        section(data-transition-speed="fast")
          h2 The business problem:
          blockquote 
            p Specifically, the marketers said they wanted to send specially designed ads to women in their second trimester, which is when most expectant mothers begin buying all sorts of new things, like prenatal vitamins and maternity clothing. “Can you give us a list?” the marketers asked.

        section(data-transition-speed="fast")
          h2 Discussion questions
          p Put yourself in the role of an analyst at Target.  
            ul
              li What data do you need to build a model to predict if a woman is pregnant? 
              li Is it ethical to merge data from outside of Target's scope in customers' lives (<i>e.g.</i> public birth records).
              li Why did people get upset?
              li Does the cost of backlash outweigh the benefits of the knowledge?
              li How might Target have met the business goals, without losing customer credibility?
              li To what degree should Andrew Pole be held accountable for how his analyses were used?

          aside.notes Also linked to your Guest ID is demographic information like your age, whether you are married and have kids, which part of town you live in, how long it takes you to drive to the store, your estimated salary, whether you’ve moved recently, what credit cards you carry in your wallet and what Web sites you visit. 

        section(data-transition-speed="fast")
          h2 How should we think about ethics?
          img.right(src="images/ethics.png" style="width: 38%")
          p When we are faced with a data mining application, what framework should we use to think about ethics?

        section(data-transition-speed="fast")
          h2 Ethical decision points
          p Ethical decision points provide a framework for exploring the relationship between what values you hold as individuals—and as members of a common enterprise—and aligning those values with the actions you take in building and managing products and services utilizing big data technologies. 
            ol
              li <b>Inquiry</b>: what are are my organization's values? What are my values?
              li <b>Analysis</b>: what are the practice standards in my industry?
              li <b>Articulation</b>: where are there alignments and gaps between our values and proposed data practices?
              li <b>Action</b>: what must we do to close value alignment gaps?

        section(data-transition-speed="fast")
          h2 Inquiry: what are my values?
          p Target has a #[a(href="https://corporate.target.com/about/mission-values") values page] on their website, but more relevant to this discussion is the #[a(href="http://www.target.com/spot/privacy-policy") privacy policy].
            blockquote At Target, we want you to know how we collect, use, share and protect information about you. By interacting with Target, you consent to use of information that is collected or submitted as described in this privacy policy. We may change or add to this privacy policy, so we encourage you to review it periodically. To help you track the changes, we include a history of changes below.

        section(data-transition-speed="fast")
          h2 Analysis: what are the standards of my industry?
          p In marketing a great place to start is the Digital Marketing Association's #[a(href="http://thedma.org/centers-of-excellence/dma-ethics-and-compliance-program/") Ethics and Compliance Program] and #[a(href="http://thedma.org/wp-content/uploads/DMA_Guidelines_January_2014.pdf") Guidelines for ethical business practice] the latter of which covers privacy and marketing to children, something Target did whether they knew it or not, in depth.

        section(data-transition-speed="fast")
          h2 Articulation: are there value gaps? 
          blockquote If we send someone a catalog and say, ‘Congratulations on your first child!’ and they’ve never told us they’re pregnant, that’s going to make some people uncomfortable,” Pole told me. “We are very conservative about compliance with all privacy laws. But even if you’re following the law, you can do things where people get queasy.
          p The girl was high school age, a fact that, presumably, Target knew. Why then did they choose to proceed with the mailer?

        section(data-transition-speed="fast")
          section(data-transition-speed="fast")
            h2 Action: how do we close the value gaps?
            blockquote With the pregnancy products, though, we learned that some women react badly. Then we started mixing in all these ads for things we knew pregnant women would never buy, so the baby ads looked random. We’d put an ad for a lawn mower next to diapers. We’d put a coupon for wineglasses next to infant clothes. That way, it looked like all the products were chosen by chance.

          section(data-transition-speed="fast")
            p ... and then of course they say this dehumanizing thing.
            blockquote And we found out that as long as a pregnant woman thinks she hasn’t been spied on, she’ll use the coupons. She just assumes that everyone else on her block got the same mailer for diapers and cribs. As long as we don’t spook her, it works.

        section(data-transition-speed="fast")
          h2 Ethical behavior requires constant diligence 
          p The ethical consequences of data mining isn't a one-time consideration. Everytime we sit down to conduct and analysis we should think about the potential for harm.  We'll return to the discussion of ethics repeatedly throughout the course.

        section#R_intro
          h2 R for statistical computing
          figure.right(style="width: 35%")
            img(src="images/r-usage-salary.png" style="width: 100%")
            figcaption From O'Reilly's 2014 survey of #[a(href="http://www.oreilly.com/data/free/2014-data-science-salary-survey.csp") data science salaries].
          p R is a leading tool in the industry for statistics and data science.
            ul(style="display: block")
              li is free and open-source
              li has an active community, which means you've got tons of analysis options.
              li is in demand!

        section(data-transition-speed="fast")
          h2 Installation
          p 
            ol
              li Install #[a(href="http://cran.cnr.berkeley.edu/") R]
              li Install #[a(href='http://www.rstudio.com/products/rstudio/download/') R Studio]
 
        section
          h2 Project 1
          p
            a(href="https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%201/problem_statement_project_1.md") Project 1 
            will explores data from Washington D.C.'s Capital Bikeshare program. We want to identify the best predictors of a successful bike station. To complete the project you'll need to:
          ul
            li formalize the business problem as a data mining process
            li compute the outcome (rentals per day)
            li use aggregation and summaries to create model inputs
            li analyze and interpret your results
            li write a brief report of your findings

        section(data-transition-speed="fast")
          h2 Project 1 data
          p I've constructed a dataset from a couple different sources that includes
            ul 
              li usage data in 2012
              li daily weather in 2012
              li station details such as long/lat, and nearby amenities

        section
          h2 Week 1 Resources
          ol
            li #[a(href="http://www.dcc.fc.up.pt/~ltorgo/DataMiningWithR/") Data Mining with R: Learning with Case Studies]
            li #[a(href="http://www.amazon.com/Ethics-Big-Data-Balancing-Innovation/dp/1449311792") Ethics of Big Data: Balancing Risk and Innovation]
            li #[a(href="https://www.coursera.org/course/rprog") Coursera course on R]
            li #[a(href="https://www.coursera.org/course/exdata") Exploratory Data Analysis from Coursera]
            li #[a(href="https://www.udacity.com/course/ud651") Exploratory Data Analysis from Udacity]
            li #[a(href="http://www.mmds.org/") Mining of Massive Datasets]
            li The Elements of Statistical Learning. Trevor Hastie, Robert Tibshirani and Jerome Friedman. This is a classic text available #[a(href="http://statweb.stanford.edu/~tibs/ElemStatLearn/") from their website]
            li #[a(href="http://onepager.togaware.com/") Hands-On Data Sciece with R]
            li #[a(href="http://datamining.togaware.com/") Data mining resources]

        section#week2(data-background="images/data_miner2.jpg" data-background-size="100%")
          h2 Week 2: Linear regression
          h4 BI Tech CP303 - Data Mining
          h4 Erin Shellman 

        section
          h2 How many rentals per day?
          img.right(src="images/crosswalk_lm.png" style="width: 40%")
          p Predicting the expected number of bike rentals from a station per day is useful in lots of applications:
            ul
              li Allocating bikes across stations
              li Budgeting for monthly bike maintenance
              li Picking the best locations for new stations
          p We can use ordinary least squares (OLS) regression to estimate real-valued outcomes such as number of rentals per day.  

        section(data-transition-speed="fast")
          h2 Why linear regression?
          p Linear regression is a relatively simple method for predicting real-valued variables, but it's great to have in your toolbelt due to the ease of interpretability and implementation, power, and flexibility.

        section(data-transition-speed="fast")
          h2 Interpretability
          p Regression coefficients have a straight-forward interpretation as the average change in the outcome for a unit change in an explanatory variable, all else equal. The coefficients also give a rough indication of what variables contribute most to the predicted value, which can be helpful in understanding causal mechanisms and for variable selection.

        section(data-transition-speed="fast")
          h2 Ease of implementation
          p Virtually all programming languages have implementations for linear regression, so they can be applied across many applications.

        section(data-transition-speed="fast")
          h2 Powerful
          p In prediction settings, linear models can outperform more complex nonlinear techniques, particularly when working with small training sets or sparse data. This is because regression is an averaging technique, making it less sensitive to missingness and extreme values.

        section(data-transition-speed="fast")
          h2 Flexible
          p Even in situations where linearity assumptions fail, regression can be applied on transformations of variables such as logarithms. 

        section(data-transition-speed="fast")
          h2 Mathematical foundations
          p Let's walk through the foundations of regression with just one explanatory variable/#[a(href="http://en.wikipedia.org/wiki/Covariate") covariate]. Regression with just one covariate is called #[a(href="http://en.wikipedia.org/wiki/Simple_linear_regression") simple linear regression]. The reasoning and intuition behind an SLR is simple to understand and generalizes to regression with multiple covariates.

        section(data-transition-speed="fast")
          h2 Mathematical foundations
          p SLR describes the relationship between an explanatory variable (or <i>independent</i> variable) and an outcome variable (sometimes called the <i>response</i> or <i>dependent variable</i>). It answers the question, how can we express the expected number of rentals as a function of the number of nearby crosswalks?
          img.center(src="images/crosswalk_lm.png" style="width: 40%")

        section(data-transition-speed="fast")
          h2 Mathematical foundations
          p Given $n$ observations of $x$ and $y$,
          | $ (x_1, y_1), (x_2, y_2), \ldots , (x_n, y_n) $
          p the model for a simple linear regression is expressed as
          | $ y_i = \beta_{0} + \beta_{1}x_i + \epsilon_i $
          p(class="fragment current-visible") Hey, kinda looks like $y = mx + b$ where $\beta_0$ is the intercept and $\beta_1$ is the slope of the line.
            img.right(src="images/Linear_Function_Graph.svg" width="30%" height="30%")

        section(data-transition-speed="fast")
          h2 Which line is the best?
          p How do we choose $\beta_0$ and $\beta_1$? With visual inspection alone, there seems to be several lines that could be drawn through the center of the points. We need an algorithm for standardizing and automating the process of finding the best one. 
          img.center(src="images/three_lines.png" width="50%" height="50%")

        section(data-transition-speed="fast")
          h2 Errors
          img.right(src="images/residuals.png" width="40%" height="40%")
          p Data points will not fall exactly on the straight line. The distance of a point from the fitted line is called the error (or residual), $ \epsilon_i $. The errors are assumed to be independent and normally distributed with mean 0 and standard deviation $\sigma$.

        section(data-transition-speed="fast")
          h2 Ordinary least squares ftw
          p #[a(href="http://en.wikipedia.org/wiki/Ordinary_least_squares") Least squares] selects the line that minimizes the sum of the squares of the errors.
          | $ \text{minimize} \sum\limits_{i=1}^n (y_i - \beta_{0} - \beta_{1}x_i)^2 $

        section(data-transition-speed="fast")
          h2 Minimize the squared errors
          p
            | $ \sum\limits_{i=1}^n\epsilon_i = \sum\limits_{i=1}^n (y_i - \beta_{0} - \beta_{1}x_i)^2 $
          p Choose $\beta_0$ and $\beta_1$ so that $\sum\limits_{i=1}^n\epsilon_i^2$ is minimized.

        section(data-transition-speed="fast")
          h2 Intuition
          p Let's think about what the components of the regression equation represent to gain an intuition. The vector $y_i$ contains the observed values of the outcome variable. $\beta_{0} + \beta_{1}x_i$ is the estimated, or expected, $y_i$ value. So when we minimize the sum of squared errors we're making $(observed - expected)^2$ as close to 0 as possible.

        section(data-transition-speed="fast")
          section(data-transition-speed="fast")
            h2 Assumptions
            p To safely apply linear regression without fear of weird results, some assumptions must be met:
              ol
                li Linearity between the outcome and covariates
                li Independence of the errors (no #[a(href="http://en.wikipedia.org/wiki/Autocorrelation") autocorrelation])
                li Constant variance (#[a(href="http://en.wikipedia.org/wiki/Homoscedasticity") homoscedasticity]) of the errors
                li Normality of errors

            p We might violate some assumptions and it's application-dependent whether we should be concerned about those violations.

          section(data-transition-speed="fast")
            h2 Diagnostic plots
            p In R, if you plot a linear model object, it'll give you a nice collection of diagnostic plots for assessing model assumptions.
            pre
              code.r(data-trim, contenteditable).
                model = lm(rentals ~ crossing, data = data)
                par(mfrow = c(2, 2)) # make 4 plots in 2 rows, 2 cols
                plot(model)
            img.center(src="images/diagnostic_plot.png" width="50%" height="50%")

        section(data-transition-speed="fast")
          h2 Interpretation
          p Regression equations have a nice interpretation. The intercept $\beta_0$ is the mean value of $y$ when $x = 0$. The slope of the regression line, $\beta_{1}$, is the change in the mean of y for each unit change in x.

        section(data-transition-speed="fast")
          h2 R regression output 
          p The regression equation is 
          | $ rentals = 27.53 + 0.49 * crosswalks $.
          p When there are no crosswalks the average number of bike rentals is about 28. For each additional crosswalk, we can expect an additional half rental. Or, for every two additional crosswalks, we can expect another rental.
          pre
            code.r(data-trim, contenteditable).
              model = lm(rentals ~ crossing, data = data)
              summary(model)

              Call:
              lm(formula = rentals ~ crossing, data = data)

              Residuals:
                  Min      1Q  Median      3Q     Max 
              -51.815 -21.176  -8.394  13.765 126.969 

              Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
              (Intercept) 27.53163    2.59354  10.615  < 2e-16 ***
              crossing     0.49159    0.07031   6.992 4.92e-11 ***
              ---
              Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

              Residual standard error: 29.54 on 183 degrees of freedom
              Multiple R-squared:  0.2108,	Adjusted R-squared:  0.2065 
              F-statistic: 48.88 on 1 and 183 DF,  p-value: 4.92e-11

        section
          h2 Assessing model quality
          p We've described regression as method for numerical prediction, and used the regression of crosswalks on rentals as an example. Now we want to use that model to predict expected rentals. How do we know if our model is any good? 

        section(data-transition-speed="fast")
          section(data-transition-speed="fast")
            h2 Training and testing sets
            p We measure prediction quality by simulating new data coming in. We do that by dividing the original data into a <i>training set</i> and a <i>testing set</i>. The <i>training set</i> is used to fit and tune the predictive model. Then, the <i>testing set</i> is used to validate the predictions. Generally construction of the train and test sets should be done completely at random.

          section(data-transition-speed="fast")
            h2 Quick heuristics for study design
            p  
              ol.left large sample size
                li 60% train
                li 20% test
                li 20% validation
              ol.center medium sample size
                li 60% train
                li 40% test
              ol.right small sample size
                li Do cross validation

        section(data-transition-speed="fast")
          section(data-transition-speed="fast")
            h2 Measuring model quality for continuous outcomes
              p 
                | $ \text{mean squared error (MSE)} = \frac{1}{n} \sum\limits_{i=1}^n (prediction_i - observed_i)^2 $
              p
                | $ \text{root mean squared error (RMSE)} = \sqrt{\frac{1}{n} \sum\limits_{i=1}^n (prediction_i - observed_i)^2} $

          section(data-transition-speed="fast")
            h2 Which metric should I use?
            p Just try them all, each metric has strengths and weaknesses.
              ul
                li MSE and RMSE are sensitive to outliers.
                li Median absolute deviation is often more sensitive to outliers.

        section
          h2 Week 2 Resources
          ol
            li #[a(href="https://class.coursera.org/predmachlearn-012") Practical Machine Learning] is a great Coursera course and is part of the data science specialization from Johns Hopkins.
            li #[a(href="http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf") A Short Introduction to the caret Package]
            li #[a(href="http://www.jstatsoft.org/v28/i05/paper") Building Predictive Models in R Using the caret Package]

        section#week3(data-background="images/data_miner3.jpg" data-background-size="100%")
          h2 Week 3: Linear regression extensions
          h4 BI Tech CP303 - Data Mining
          h4 Erin Shellman 

        section
          h2 Week 3 Resources
          ol
            li #[a(href="http://research.google.com/pubs/pub43146.html") Machine Learning: The High Interest Credit Card of Technical Debt]

        section#week4(data-background="images/data_miner4.jpg" data-background-size="100%")
          h2 Week 4: Classification - Logistic Regression 
          h4 BI Tech CP303 - Data Mining
          h4 Erin Shellman 

        section#classification
          h2 Classification
          p We use classification techniques when we're trying to assign something to a category.
          ul 
            li Spam or not spam
            li Diseased or not diseased
            li Fraud or not fraud
            li Bot or not a bot

        section(data-transition="linear" data-transition-speed="fast")
          h2 Identifying spam
          p We can construct a decent spam classifier using simple linear regression using the number of occurrences of the character '$' as our predictor.
          img(src="images/is_it_spam.png")

        section(data-transition="linear" data-transition-speed="fast")
          h2 Identifying spam
          p Our decision boundary is at $ y = 0.5 $ and with regression we derive the rule that $ x \geq 7.7 $ indicates spam.
          img(src="images/is_it_spam_2.png")

        section(data-transition="linear" data-transition-speed="fast")
          h2 Identifying spam
          p Our classifier starts misclassifying when we add another observation with a relatively extreme number of $'s.
          img(src="images/is_it_spam_3.png")

        section(data-transition-speed="fast")
          section
            h2 We need a new tool
            p Our current tool, linear regression, is not going to work in general for a classification application.  Instead of linear regression we're going to use a regression in a family called generalized linear models (GLMs), <b>logistic regression</b>.
          section
            h2 History of GLM
            p Articulated in Nelder and Wedderburn in 1972 paper in Journal of the Royal Statistical Society.
            p GLMs are composed of three parts:
              ol
                li An exponential family model for the response
                li A systematic component via a linear predictor
                li A link function that connects the means of the response to the linear predictor

          section 
            h2 Logistic regression GLM parts
            ol 
              li Exponential family model:
              li Linear predictor: $ \eta_{i} = \sum_{k = 1}^{p} X_{ik}\beta_k $
              li Link: $ g(\mu) = \eta = \log{\mu \over 1-\mu} $

        section
          p A logit model, or logistic regression is used to model the probability of binary events. The log odds of the outcome is modeled as a linear combination of the predictor variables.
          // Show equation here?

        section
          h2 The logistic (or sigmoid) function
          p $ F(t) = {e^{t} \over e^{t} + 1} = { 1 \over 1 + e^{t}} $

        section
          h2 What are we actually doing here?

        section
          h2 How do we decide the class?
          p As we've seen, the outcome of a logistic regression is a probability but how do we classify?

        section
          h2 Week 4 Resources
          ol
            li #[a(href="http://www.ats.ucla.edu/stat/r/dae/logit.htm") R Data Analysis Examples: Logit Regression]

        section#week5(data-background="images/data_miner5.jpg" data-background-size="100%")
          h2 Week 5: Classification - Trees
          h4 BI Tech CP303 - Data Mining
          h4 Erin Shellman 

        section
          p A “divide-and-conquer” approach to the problem of learning from a set of independent instances leads naturally to a style of representation called a decision tree.

        section
          p steps of a tree classification method:
            ol
              li iteratively split variables into groups
              li Evalulate "homogeneity" with each group
              li Split again if necessary
          p Pros are easy interpretation and good performance in nonlinear setting
          p Cons are potential for overfitting, can be high variance, harder to estimate uncertainty

        section
          p basic algoritm
            ol
              li Start with all variables in one group
              li find the variable/split that best separates the outcomes
              li divide the data into two groups (leave) on that split (node)
              li within each split, find the best variable/split that separtes the outcomes
              li continue until the groups are two small or sufficiently "pure"

          p think about where the decisions in this alogoritm need to be made.
            ul
              li at the splits based on some criteria for 'best' or 'most homogeneous'
              li at the point of determining node 'purity'

        section
          h2 measures of impurity
          p Misclassification error - 0 perfect purity, 0.5 no purity
          p Gini index - 0 perfect purity, 0.5 no purity
          p Deviance/information gain - 0 = perfect purity 1 = no purity

        section
          section
            h2 Other methods of classification
            p We focused on two common methods of classification, logistic regression and classification trees, but there are many methods we could have chosen from depending on our application.

          section
            h2 Support Vector Machines
            p Quick description and code in caret

          section
            h2 Neural networks
            p Quick description and code in caret
              
        section
          h2 Week 5 Resources

        section#week6(data-background="images/data_miner6.jpg" data-background-size="100%")
          h2 Week 6: Association rule mining
          h4 BI Tech CP303 - Data Mining
          h4 Erin Shellman 

        section
          h2 the million song dataset
          p http://blog.echonest.com/post/3639160982/million-song-dataset

        section
          h2 Week 6 Resources

        section#week7(data-background="images/data_miner7.jpg" data-background-size="100%")
          h2 Week 7: Clustering
          h4 BI Tech CP303 - Data Mining
          h4 Erin Shellman 

        section(data-transition-speed="fast")
          h2 Unsupervised learning
          p Sometimes you won't really know what your outcome is.
          p in that case to build a predictor
          ol 
            li create clusters
            li name clusters
            li build predictor for the cluster

        section(data-transition-speed="fast")
          h2 Week 7 Resources
          ol
            li Nice presentation by #[a(href="http://www.slideshare.net/PhamCuong/clustering-technique-for-collaborative-filtering-recommendation-and-application-to-venue-recommendation") Pham Cuong] about clustering techniques for recommendations.

        section#week8(data-background="images/data_miner8.jpg" data-background-size="100%")
          h2 Week 8: Time series 1
          h4 BI Tech CP303 - Data Mining
          h4 Erin Shellman 

        section
          h2 Week 8 Resources

        section#week9(data-background="images/data_miner9.jpg" data-background-size="100%")
          h2 Week 9: Time series 2
          h4 BI Tech CP303 - Data Mining
          h4 Erin Shellman 

        section
          h2 Week 9 Resources

        section#week10(data-background="images/data_miner10.jpg" data-background-size="100%")
          h2 Week 10: Sharing is caring!
          h4 BI Tech CP303 - Data Mining
          h4 Erin Shellman 

        section(data-transition-speed="fast")
          h2 Share your work!
          p You've learned a ton about data mining and solving problems with data, and now to make the most of your efforts lets create digital portfolios of our work with Github and Markdown.

        section(data-transition-speed="fast")
          h2 Intro Git

        section(data-transition-speed="fast")
          h2 Intro GitHub

        section(data-transition-speed="fast")
          h2 Intro Markdown

        section(data-transition-speed="fast")
          h2 You want some more?
          p Data Kind
          p Kaggle Competitions

        section(data-transition-speed="fast")
          h2 Week 10 Resources
          ol
            li #[a(href="http://www.noamross.net/blog/2013/1/7/collaborating-with-r.html") Noam Ross post]

        section#acknowledgements
          h2 Thanks buddies!
          p Creating this course wasn't easy and I couldn't have done it alone. Huge thanks to my TA Ryan Brush who encouraged and recommended me to teach this class. Big ups to #[a(href="http://brianbeck.com/") Brian Beck] who styled the lecture notes. 
          p Thank you to the #[a(href="https://www.flickr.com/photos/boston_public_library/sets/72157627774101436") Boston Public Library] for posting all the awesome old coal mining postcards on Flickr!

        section
          section#author
            h2 But who <i>is</i> #[a(href="http://www.erinshellman.com/") Erin Shellman]?  

            p I’m a programmer and statistician working as a research scientist at #[a(href="http://aws.amazon.com/") Amazon Web Services], and before that I was a Data Scientist in the Nordstrom Data Lab. I mostly code in Python and R, and increasingly use Javascript for visualization and data retrieval through APIs. I’m rigorous, ask good questions and sometimes even answer those questions. I love to teach and speak, and do both often through talks and as co-organizer of #[a(href="http://www.meetup.com/Seattle-PyLadies/") PyLadies-Seattle].

          section
            p #[a(href="mailto:rtbrush@uw.edu") Ryan Brush] is a data management program manager with Microsoft's Global Foundation Services division. This division delivers the core infrastructure and foundational technologies for Microsoft’s 200-plus online businesses, including Bing, Hotmail, MSN, Office 365, Xbox Live and the Windows Azure platform. Brush has more than seven years of experience in enterprise system development with an emphasis on building scalable data and reporting solutions. His experience includes requirements gathering, creating metric road maps, improving data quality and developing data analytics platforms. He also has experience writing SQL queries against large-scale, complex data sets. Prior to working at Microsoft, Brush was a business intelligence consultant with various organizations in the cloud computing, travel and high-tech industries.

    script(src="//cdnjs.cloudflare.com/ajax/libs/foundation/4.1.2/js/foundation.min.js")
    script(src="lib/js/head.min.js")
    script(src="js/reveal.min.js")

    script.
      Reveal.initialize({
        width: 1200,
        height: 900,

        controls: true,
        progress: true,
        history: true,
        center: true,
        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none
        math: {
          mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
          //mathjax: 'js/MathJax.js',
          config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        }, 
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'plugin/math/math.js', async: true },
          { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        ]
      });
